---
title: "Introduction to Applied Predictive Modeling"
author: "Michael Fichman"
date: "2025-01-30"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    code_download: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# 1. Introduction

The world is awash in complex predictive tools and analytics. Some of these tools are very useful for planners and others concerned with social and spatial phenomena. The best way to develop utility with these kinds of tools is by starting with the basics - regression models. These models are interpretable, simple, and often deceptively powerful.

This is a beginner's guide to predictive analytics. It introduces a few simple model workflows and basic concepts to users who have little or no background working with statistics and/or computer coding. It is NOT a comprehensive text on regression models. Such texts are many in number, but if you want to get into one - I quite like [Regression and other Stories, by Gelman et al](https://avehtari.github.io/ROS-Examples/examples.html#Introduction).

Each section in this document details a different regression model we will use this semester in Land Use and Environmental Modeling (CPLN 6750 and the University of Pennsylvania Weitzman School of Design), and provides simple workflows in R using sample data sets.

**The learning objectives are as follows:**

1. Understand what it means to "model" data using regression.

2. Understand how regression models can be used to estimate outcomes, and learn how model selection is related to the outcome of interest.

3. Observe and understand the training and testing workflow used to validate models in R.

4. Learn basic terminology and statistics associated with OLS and binomial logit regression.

5. Understand the notion of model "accuracy" and "generalizability"

6. Learn how feature engineering and spatial data can be used to improve model quality.

## 1.1. Explanation versus Prediction

You very often see regression models used in scholarly papers to show the "effect of A on B" or the "association between A and B"... "all else equal." In a public policy or planning context, causal and associative applications of regression are very useful for measuring policy effects or understanding social scientific relationships in a statistical framework. This is NOT what we are going to do. You will not hear any "A causes B" in this text.  Furthermore, we are not trying to use controls to isolate the associations between A and B, "all else equal." What this means is that we are going to violate some assumptions (e.g. the "rules") that will make econometric users of regression absolutely cringe.

We are going to use regression models for a different purpose - to estimate outcomes (dependent variables) given a bunch of predictors (independent variables). To give you an example of the difference in approach, if I want to estimate the heights of a number of people, I could do so very accurately if I fed a regression model information about their wingspans. However, that can't tell me anything about WHY a person is so tall.

The key to making above average predictive models is finding good independent variables and harnessing the power of their correlation with the dependent variable to make predictions. They key to making great predictive models is "engineering" good "features" to make models that are both accurate and flexible.

## 1.2. Model Types

**We are going to demonstrate the use of two types of models**, each is designed to suit a different type of outcome variable. First, we will use **Ordinary Least Squares (OLS) regression** to estimate normally distributed, continuous outcomes. Second we will use **binary logistic regression** to estimate 0/1 outcomes (e.g. estimating the probability of an outcome being a 1). Lastly, we will do some **feature engineering using spatial data for OLS**.

*Your choice of model depends on the use case - is the outcome you are interested in binomial or continuous?*

There are other models for different outcome types not covered here, including Poisson regression for count data.

## 1.3. How Do Predictive Models "Work?"

All of these models are built around a simple framework - making a sort of "best fit" line through your data. You've certainly seen an x-y scatterplot with a "best fit" line plotted through the middle. This is actually a simple regression model - an OLS model fits a function that predicts the value of Y for a given X. Functionally, it minimizes the distance between the observations (the error) and the best fit line.

```{r echo=FALSE, message=FALSE, include=FALSE}

library(tidyverse)

# Set seed for reproducibility
set.seed(123)

# Generate random x-y data
data <- tibble(
  x = rnorm(100, mean = 50, sd = 10),  # 100 random x values from normal distribution
  y = 2 * x + rnorm(100, mean = 0, sd = 15)  # y follows a linear trend with some noise
)

# Plot the data with a best-fit line
ggplot(data, aes(x = x, y = y)) +
  geom_point(color = "blue", alpha = 0.6) +  # Scatter plot points
  geom_smooth(method = "lm", color = "red", se = TRUE) +  # Best-fit line with confidence interval
  theme_minimal() +
  labs(title = "Randomly Generated X-Y Data with Best Fit Line",
       x = "X Values",
       y = "Y Values")

```

Each of the models in this text apply a different fitted function to your data, but in the end, the predictive functionality is similar - they all output the "average" estimate given all the independent variables in the model.

## 1.4 How Do You Assess Model Performance?

How do you know if your predictive model is doing a good job? We do this by "validating" the model. We hold out a sample of data where we know the actual outcome, and see if our model can accurately estimate the outcome.

We can split our data. We "train" our model using a portion of our data, and hold out a "test" set with all the same characteristics. When we "show" our model the independent variables from our test set, we can make predictions

We will have a few ways to judge that:

- Is the model accurate on unseen data? Are the predictions close to the known values in our test set?

- Does the model performance generalize across contexts? Do we have big errors with some particular category of observation and small ones with another?

The "error metrics" vary depending on what type of model you are using, as we will see.

## 1.5. R Packages

We will use a few basic packages in this text.

If you do not have any of the packages listed below installed on your computer, install them like so:

```{r install_pkg, eval=FALSE, include=TRUE}
install.packages('tidyverse')
```

If you have already installed a package on your computer, you can load it into your environment using the `library` command.

`tidyverse` is the main family of data wrangling and visualization packages in R, including `ggplot2` and `dplyr`

`sf` is the primary vector GIS package in the tidyverse, short for "Simple Features"

`caret` is a package with tools for predictive modeling - training and testing models.

`readr` is a package for reading in some data types, like Excel files.

`plotROC` is a package used to create an ROC curve - a diagnostic test for logit models.

```{r load_pkg , message = FALSE, warning = FALSE}
library(tidyverse)
library(sf)
library(caret)
library(readr)
library(plotROC)
```


# 2. Ordinary Least Squares Linear Regression

OLS regression, or multiple linear regression, is a simple model that can estimate the value of a continuous outcome - like prices or quantities. It's not good for count data, like numbers of incidents. It is simple, interpretable, and widespread.

You've very likely seen "best fit" lines applied to an x-y scatterplot. The line is plotted to minimize the distance between the points and the line - that difference is the "error" associated with this estimate of the relationship. That's essentially what regression does, but fitting multiple lines for multi-dimensional data.

```{r, message = FALSE}
# Set seed for reproducibility
set.seed(123)

# Generate random x-y data
data <- tibble(
  x = rnorm(100, mean = 50, sd = 10),  # 100 random x values from normal distribution
  y = 2 * x + rnorm(100, mean = 0, sd = 15)  # y follows a linear trend with some noise
)

# Plot the data with a best-fit line
ggplot(data, aes(x = x, y = y)) +
  geom_point(color = "blue", alpha = 0.6) +  # Scatter plot points
  geom_smooth(method = "lm", color = "red", se = FALSE) +  # Best-fit line with confidence interval
  theme_minimal() +
  labs(title = "Randomly Generated X-Y Data with Best Fit Line",
       x = "X Values",
       y = "Y Values")

```


## 2.1. Data set

We are going to use a [data set from the World Bank](https://www.kaggle.com/datasets/patelris/crop-yield-prediction-dataset?select=yield.csv) to see if we can predict national crop yields. This data set consists of agricultural yields for many types of crops in over 100 countries over a series of years. There is accompanying info about yearly rainfall and temperature. I have cleaned these data a bit to consist only of a few years. 

Let's say we are interested in food system planning, and we want to be able to predict the yield (weight per hectre) for several crop types - maize, wheat, etc., under several scenarios for the coming year related to rainfall and temperature - a hot year, a rainy year, etc.,

```{r load data, warning = FALSE, message = FALSE}
yield_df <- read_csv("~/GitHub/R_FAQ_For_Planners/modeling/data/yield_df_clean.csv")
```

## 2.2. Exploratory Analysis

The first step is always to do a few quick pieces of exploratory data analysis - looking at the data types in your set, the distributions of the data, and the correlations between different variables.

### 2.2.1. Data types

The `glimpse` command from `dplyr` in the `tidyverse` is the best way to quickly get a look at your data. Here we see that we have rows that have a column id, a country, a crop type, a year, yield per hectare (our continuous dependent variable - `yield_hg_per_ha`)

```{r}
glimpse(yield_df)
```
### 2.2.2. Distributions

We can look at how our continuous variables are distributed. Some of them are similar - very right-tailed (including our dependent variable). Others are a bit all over the place.

```{r, warning = FALSE, message = FALSE}

yield_df %>%
  select(yield_hg_per_ha, average_rain_fall_mm_per_year, yield_last_year, pesticides_tonnes, avg_temp) %>%
  gather(key = "variable", value = "value") %>%
  ggplot()+
  geom_histogram(aes(value))+
  facet_wrap(~variable, scales = "free")+
  theme_minimal()


```


### 2.2.3. Correlations - Continuous Variables

Good predictive modeling is all about harnessing the power of correlation between independent predictors and your dependent variable. Take a look at how your continuous variables might be correlated with your outcome variable `yield_hg_per_ha`

A few notable observations - it seems that there is little relationship between yield and rainfall, there are some major outliers in pesticide application amounts, and that a previous year's yield by crop and country has a very, very strong relationship to current year yield.

```{r}
yield_df %>%
  select(yield_hg_per_ha, average_rain_fall_mm_per_year, yield_last_year, pesticides_tonnes) %>%
  gather(-yield_hg_per_ha, key = "variable", value = "value") %>%
  ggplot()+
  geom_point(aes(x = value, y = yield_hg_per_ha))+
  facet_wrap(~variable, scales = "free")+
  theme_minimal()
```

### 2.2.4. Correlations - Categorical variables

Let's examine how our dependent variable might vary from category to category.

This is a bit messy, but we can see that from country to country, and crop to crop, there is a lot of variation in yield. The fact that there are differences across categories means that this will probably be a good predictor in our model. If they were all the same, it wouldn't give us much predictive power.

```{r}
yield_df %>%
  select(yield_hg_per_ha, Area, Item) %>%
  gather(-yield_hg_per_ha, key = "variable", value = "value") %>%
  ggplot()+
  geom_bar(aes(x = value, y = yield_hg_per_ha), stat = "identity")+
  facet_wrap(~variable, scales = "free")+
  coord_flip()+
  theme_minimal()
```

## 2.3. A Simple Model

Here's a simple example so we can take a look at our modeling outputs. Le's start by predicting yield using only rain fall, temperature and the amount of pesticides. Look at the following code chunk to see how we do this with the `lm` function.

The `lm` function takes a data set (`yield_df`), and an equation with a dependent variable (`yield_hg_per_ha`) followed by a `~` (called a "tilda").  You list the independent variables on the other side of the tilda.

The lm function will output a model object, which we call `reg1`.

```{r}

reg1 <- lm(data = yield_df, 
           yield_hg_per_ha ~ 
             average_rain_fall_mm_per_year + avg_temp + pesticides_tonnes)
```

You can use the `summary` command to look at the model outputs.

The model summary shows us a lot of information. Let's just focus on a three items of information - Beta Coefficients, p-values, and R-Squared.

**The beta coefficients aka `Estimate`** - these show the strength (big or small) and the direction (positive or negative) of the relationship between an independent variable and our dependent variable (yield) given the other variables in the model. Here is an example of how they are interpreted using technical language. "On average, a one unit increase in `average_rain_fall_mm_per_year` is associated with a 5.335 unit increase in `yield_hg_per_ha`, all else equal."

**The p-value `Pr(>|t|)`** represents the probability that this estimate is significantly different from zero (e.g. how much confidence do we have in the coefficient). A low p-value means a relatively high confidence in the estimate. Here it is `0.0773` - this is very low, but not low enough to show significance of the relationship in the eyes of, say, a medical journal reviewer. For our purposes though, a 7% chance the estimate is invalid, that's means there's a good chance this is a useful relationship for prediction.

The **R-Squared** value tells us the amount of variance in the dependent variable that is explained by this combination of independent variables. This isn't the same as an "accurate" model, but higher values generally relate to more powerful predictive models. Here, the value is 0.02 (on a scale of 0-1) - which is horrible.

```{r}
summary(reg1)

```

**Do we care about the beta coefficients, the p-values and the R-squared?** 

Only a little bit - but nowhere close to as much as most people do when they use regression. We want to know whether these variables in this model predict yields well - we don't care about explaining their effects in isolation. We can use these as diagnostics as we are building our model - adding features and engineering new ones. The real metrics we care about are discussed later.

## 2.4. Fixed Effects

It would stand to reason that the model "knowing" what country and crop type you're predicting yields for would help. Some countries are probably easier than other to grow crops. Our exploratory analysis told us that yields vary a lot across crop and country.

Let's add these "categorical" variables to our model.

When you add a categorical variable, like country (the variable `Area`), it's known as a "fixed effect" or a "dummy variable" - where each category is given it's own line item in the regression summary, complete with an estimate. One level of the fixed effect is not represented in the summary - so there's one country you don't see here. Each estimate and p-value relates to that "base case" hold out.

Let's create a new regression, called `reg2`. In this regression - each coefficient represents the unit change in crop yield associated with being in Algeria relative to being in Albania (the hold out country), or the yield associated with Maize relative to Cassava.

Adding these fixed effects made our R-Squared go wayyyy up - if we know the rainfall, country, crop and pesticide application, we can explain almost 80% of the variance in crop yields. Pretty good - but not quite a prediction.

```{r}

reg2 <- lm(data = yield_df, 
           yield_hg_per_ha ~ 
             average_rain_fall_mm_per_year + avg_temp + pesticides_tonnes + Item + Area)

summary(reg2)

```

Let's try a different approach - we saw there is a very strong relationship between yield and `yield_last_year` - what if we took the country out of the model, and put this in.... wow. That's a very strong predictor and a very high R-squared.

You can see that it basically knocks rainfall, temperature and pesticides out of the model in terms of significance (p-values). This is because these things are "multi-colinear" - a sign that the coefficient values are not very reliable when these variables are in the model together. This is a major way that prediction and explanation are very different when you use OLS models - we are violating some important stats assumtions.

```{r}

reg3 <- lm(data = yield_df, yield_hg_per_ha ~ average_rain_fall_mm_per_year + 
                       avg_temp + pesticides_tonnes + Item + yield_last_year)

summary(reg3)

```


## 2.5. Training and Testing

OK, now let's set up a prediction workflow. We do this by cutting up our data into a "training" and "testing" set - we "train" the model on one set, and then use it to predict on the "test" set where we know the yields already. The difference between the observed and predicted yields is our error.

### 2.5.1. Splitting our Data

We use the `set.seed` command to generate a random number sequence. We then use the `createDataPartition` command from the `caret` package to split our data. The `p` argument takes a number - here we say `.70` to indicate that we want 70% of our data in the training set. There's no set rule for the split size. The `paste` command is where we specify categorical variables - we want to make sure that single observations of categories end up in our test set. Why? If we train a model and it hasn't "seen" one of our countries, and then we "show" it that country in the test set, it won't be able to make a prediction for it.

We create two data frames - `yield_training` and `yield_test`

```{r}
set.seed(1234)

inTrain <- createDataPartition(
              y = paste(yield_df$Area), 
              p = .70, list = FALSE)


yield_training <- yield_df[inTrain,] 
yield_test <- yield_df[-inTrain,]  
```

### 2.5.2. Creating a Regression

Let's put our independent variables from `reg3` - the one with the year lag - into an `lm` object using `yield_training`, our training set. We'll name this `reg.training_1`.

```{r}
reg.training_1 <- 
  lm(data = yield_training, 
     yield_hg_per_ha ~ average_rain_fall_mm_per_year + 
                       avg_temp + pesticides_tonnes + Item + yield_last_year)

```

### 2.5.3. Predicting for our Test Set and Assessing Error

We can now use the `predict` function to "show" our test set `yield_test` to the model `reg.training_1`, and make a prediction for each row.

Our model only knows how to make predictions for a data frame that contains the exact same information it has "seen" when we trained it with`yield_training` - so your test set has to have exactly the same columns - same names, same data types - for this to work.

We want to know how much our predictions missed the mark - in absolute and percentage terms.

Here we create a new data frame called `yield_test_with_results`, we take `yield_test` and mutate some new columns - our `Prediction`, an `Error` that equals prediction minus actual, and measures of Absolute Error (`AbsError`) and Absolute Percent Error (`Abs_Pct_Error`) for each observation in the test set.

```{r}
yield_test_with_results <-
  yield_test %>%
  mutate(Prediction = predict(reg.training_1, yield_test),
         Error = Prediction - yield_hg_per_ha,
         AbsError = abs(Prediction - yield_hg_per_ha),
         Abs_Pct_Error = (abs(Prediction - yield_hg_per_ha)) / yield_hg_per_ha)
```


## 2.5.4. Accuracy and Generalizeability

To assess accuracy, we can summarize our errors - what is our Mean Absolute Percentage Error (MAPE)? What is our Mean Absolute Error (MAE)? This is very useful because it puts our errors into understandable terms - how much did we miss by? Easier to explain than R-squared!

Not bad! On average, we can predict yearly crop yields within 13%.


```{r}
yield_test_with_results %>%
  summarize(MAPE = mean(Abs_Pct_Error),
         MAE = mean(AbsError))

```

The last thing we want to do is see if our model is performing better or worse depending on the context. It would be good to know if we are making accurate predictions about, say, soybeans, but missing badly on maize.

We can group our results by `Item` and summarize our errors by crop type. As you can see, there is some variability,.

```{r}
yield_test_with_results %>%
  group_by(Item) %>%
  summarize(MAPE = mean(Abs_Pct_Error),
         MAE = mean(AbsError))

```

# 3. Binary Logistic Regression for Classification

Binary logistic regression, or "logit" is different from OLS modeling because it is designed to estimate binary outcomes. Specifically, it estimates the probability of a combination of variables you stick in a model predicts a "1" or a "0".

It does this by fitting an s-shaped function to 0/1 data, unlike OLS, which fits a linear function to continuous data.

The tricky part is that you end up with predictions that range from 0-1, and you need to figure out what threshold to use to classify your predictions, which may be values like 0.7 or 0.35, as "1" or "0".


```{r, message = FALSE}


# Generate predictor variable (x)
ex_data <- tibble(
  x = rnorm(100, mean = 0, sd = 2)  # 100 random values from N(0,2)
)

# Define a logit function with coefficients
beta_0 <- -1  # Intercept
beta_1 <- 2   # Slope

# Compute probabilities using logistic (sigmoid) function
ex_data <- ex_data %>%
  mutate(prob = exp(beta_0 + beta_1 * x) / (1 + exp(beta_0 + beta_1 * x)),
         y = rbinom(100, size = 1, prob = prob))  # Convert probabilities to binary outcome

# Fit a logistic regression model
logit_model <- glm(y ~ x, data = ex_data, family = binomial)

# Plot data with logistic regression fit
ggplot(ex_data, aes(x = x, y = y)) +
  geom_jitter(width = 0.1, height = 0.05, alpha = 0.5, color = "blue") +  # Jitter to avoid overplotting
  geom_smooth(method = "glm", method.args = list(family = binomial), color = "red", se = FALSE) +  # Logistic fit
  theme_minimal() +
  labs(title = "Simulated Logit Function with Logistic Regression Fit",
       x = "X values",
       y = "Binary Outcome (Y)")

```

## 3.1. Data set

This is a data set about tree survival published [by the aptly named Wood et al](https://datadryad.org/stash/dataset/doi:10.5061/dryad.xd2547dpw) at Michigan State. These data consist of ~2700 observations of trees, whether they are dead or alive, and a list of traits of that tree and the immediate environment.

The "use case" for our modeling example will be a forester who is interested in managing land for fire load and looking to target trees that they estimate are likely to be dead for removal and sale as timber product.

```{r, message = FALSE, warning = FALSE}
trees <- read_csv ("~/GitHub/R_FAQ_For_Planners/modeling/data/tree_morbidity.csv") %>%
  filter(is.na(Event)== FALSE) %>%
  mutate(Species = as.factor(Species),
         Event = as.factor(Event))

```

## 3.2. Exploratory Analysis

Let's take a look at our data set - our dependent variable is called `Event` - and it is binary. It's 1 if the tree is dead, and 0 if it's not.

We will take a similar approach as we did with OLS regression, examining variables in relation to one another and to the dependent variable.

## 3.2.1. Looking at our data set

```{r}
glimpse(trees)

```

## 3.2.2. Comparing continuous predictors across levels

We can use some simple `dplyr` logic to look at whether the mean values of our continuous variables tend to be different depending on whether our `Event` binary outcome is 1 or 0.

At first glance, it looks like lower concentration of lignin and phenolics in the organism is associated with death, as is time (which is time from planting to harvesting).

```{r, warning = FALSE, message = FALSE}
trees %>%
  select(Event, Time, NSC, Lignin, Phenolics, EMF, AMF) %>%
  gather(-Event, key = "variable", value = "value") %>%
  group_by(variable, Event) %>%
  summarize(mean = mean(value, na.rm = TRUE)) %>%
ggplot()+
  geom_bar(aes(y = mean, x = Event), stat = "identity")+
  facet_wrap(~variable, scales = "free")+
    theme_bw()

```

Another approach worth trying is looking at the distributions of the different variables by making histograms that are split by 0/1 and by variable.

Using this approach I can see that amongst trees that die, lignin and phenolics and NSC seem to have right tailed distributions, and there's a discrete split at a certain value. We can also see that something seems kind of weird with the "Time" variable - maybe it represents two harvest periods.

```{r, warning = FALSE, message = FALSE}
trees %>% 
    select(Event, Time, NSC, Lignin, Phenolics, EMF, AMF) %>%
    gather(-Event, key = "variable", value = "value") %>%
    ggplot()+
    geom_histogram(aes(value))+
    facet_grid(Event~variable, scales = "free")+
    theme_bw()

```


## 3.2.3. Comparing categorical predictors across levels

We can use some simple `dplyr` logic to look at whether our categorical variables vary depending on whether our `Event` binary outcome is 1 or 0.

It seems like species has a big effect - huge differences between the species in the frequency of 0's and 1's.


```{r, message = FALSE, warning = FALSE}
trees %>%
  select(Event, Soil, Sterile, Conspecific, Myco, SoilMyco, Species) %>%
  gather(-Event, key = "variable", value = "value") %>%
  count(variable, value, Event) %>%
  ggplot()+
  geom_bar(aes(value, n, fill = Event), 
           position = "dodge", stat="identity") +
  facet_wrap(~variable, scales="free") +
  coord_flip()+
    theme_bw()

```

## 3.3. A Simple Model

Let's make a simple model using the `glm` function, and setting the model `family = "binomial"` to specify a logit model. The form of the model code is otherwise the same as with `lm`.

Our model is a mix of fixed effects and continuous variables. We saw before that soil types (`Soil`) and mycorrhizal types (`Myco`) tend to be more or less redundant, so I've left one of those out of the model, figuring they "explain" the same information. 

McFadden
p-values

```{r}

reg_logit <- glm(data = trees , 
                 Event ~ Light_ISF + NSC + Lignin + Phenolics + Species + Soil,
                 family="binomial")
```

Let's examine a model summary and quickly go over some of the statistics and outputs you see there.

```{r}
summary(reg_logit)

```

The coefficients show the strength and direction of the relationship. They are interpretable in terms of the "log odds" of the dependent variable being a "1". This is not very useful when you are trying to understand relationships, so you can exponentiate the coeffecients to "un-log" them and just get plain "odds".

You can try that here:

```{r}
exp_coef <- reg_logit$coefficients %>%
  as.data.frame() %>%
  rename(coefficient = ".") %>%
  mutate(exponentiated = exp(coefficient))

```

```{r}
exp_coef

```

So for example, you'd say "All else equal, on average, the odds of a tree dying increase by x% (e.g. 1.14 is 14%) with a unit change in the independent variable."  If you have a fixed effect, you are comparing the odds between the levels of that fixed effect, "All else equal, on average, the odds of a tree of species A_____ dying are X times that of tree of species B"


The p-values are similar to those from OLS - they represent the probability that the coefficient's value is significantly different from zero - basically, our confidence the in the coefficient estimate.

The McFadden R-Squared is a "pseudo R Squared" in that it doesn't tell us the same thing as R-squared from OLS (the amount of variance in the DV that's explained by the model) but it's a useful diagnostic of model quality. Closer to 1 is stronger. That said, we are interested in predictive accuracy and generalizability, not so much in R-squared.

## 3.4. Training and Testing

## 3.4.1. Splitting our Data

This process is identical to what we did in training an OLS model.

We use the `set.seed` command to generate a random number sequence. We then use the `createDataPartition` command from the `caret` package to split our data. The p argument takes a number - here we say .70 to indicate that we want 70% of our data in the training set. There’s no set rule for the split size. The `paste` command is where we specify categorical variables - we want to make sure that single observations of categories end up in our test set.

We create two data frames - `treeTrain` and `treeTest`.

```{r}
set.seed(1234)

treetrainIndex <- createDataPartition(y = paste(trees$Species, trees$Soil), 
                                  p = .70,
                                  list = FALSE,
                                  times = 1)

treeTrain <- trees[ treetrainIndex,]
treeTest  <- trees[-treetrainIndex,]

```

## 3.4.2. Creating a Regression

Let's put our independent variables from `reg_logit` into a `glm` logit model we trained on our training set, `treeTrain`. We will call this model `logit_train`

```{r}
logit_train <- glm(data = treeTrain, Event ~ Light_ISF + NSC + Lignin + Phenolics +  
                                       Species + Soil,
                 family="binomial"(link="logit"))
```

## 3.4.3. Predicting for our Test Set

We `mutate` a new column in our `treeTest` test set that contains our estimates. We use the `predict` function to do this.

`predict` takes a few arguments - our model `logit_train`, and a data set, `treeTest`.

Our model only knows how to make predictions for a data frame that contains the exact same information it has "seen" when we trained it with`treeTrain` - so your test set has to have exactly the same columns - same names, same data types - for this to work.

```{r}

treeTest_with_results <-
  treeTest %>%
  mutate(estimate = predict(logit_train, treeTest,  type="response"))

```

The predictions are not 1's and 0's, they fall in the range 0-1 - they are probabilities of the outcome being 0 or 1.

We need to classify them according to some threshold - above that threshold, we will classify it a 1, below we will classify it a 0.

Let's examine a histogram of our estimates to see what our range of values is. We can see it's pretty bi-modal.

```{r, warning = FALSE, message = FALSE}
ggplot()+
  geom_histogram(data = treeTest_with_results, aes(estimate))+
  theme_bw()

```

## 3.4.4. Understanding Accuracy - Setting a Threshold for Classification

Remember, in our test data, we know the *actual* outcomes for each observation (`Event`). If we split our estimates along those lines, we can see that we have some high and low probabilities in both categories, but on average we seem to be getting mostly high probabilities for our 1's and low probabilities for our 0's. That's good!

In our next step, we will move forward with a threshold of 0.4 - if our estimate is above that, we will say we are classifying this an estimated `1`, otherwise, a `0`. This means we will have a lot of accurate classifications, but also some errors.

```{r, warning = FALSE, message = FALSE}
treeTest_with_results %>%
  ggplot()+
  geom_histogram(aes(estimate))+
  facet_wrap(~Event, nrow= 2)+
  theme_bw()

```

### 3.4.5. Confusion Metrics - Understanding Our 4 Outcomes

Let's create the `treeTest_with_results` dataframe again using the `predict` function, but let's add a new column called `prediction` that says "if the prediction is greater or equal to 0.4, prediction = 1, otherwise, it's a 0."

```{r}

treeTest_with_results <-
  treeTest %>%
  mutate(estimate = predict(logit_train, .,  type="response"),
         prediction = as.factor(ifelse(estimate >= 0.4, 1, 0)))
  

```

We are going to have four outcomes to our model, and it's important to be able to understand these by explaining them in plain English.

**True Positive** - We predicted the tree would die, and it did die.

**True Negative** - We predicted the tree would not die, and it did not die.

**False Positive** - We predicted the tree would die, and it did not. (An error!)

**False Negative** - We predicted the tree would not die, and it did. (Also an error!)

Our overall accuracy is the amount of True Positives and True Negatives as a percentage of total observations.

Logistic regression has a ton of (confusing) terms, among those are "sensitivity" and "specificity"

**Sensitivity** - True Positive Rate - the percentage of positives we classify correctly

**Specificity** - True Negative Rate - the percentage of negatives we classify correctly

Fittingly, there is a thing called a "Confusion Matrix" which shows us a range of statistics associated with our classification model:

```{r}
caret::confusionMatrix(treeTest_with_results$prediction, as.factor(treeTest_with_results$Event), positive = "1")

```

You can make your own version of this using `mutate` commands, and then assess model accuracy across categories.

If we look at our model performance across species, we see that our model tends to predict negatives (eg 0) for our Quercus species - be those true or false negatives. 

```{r}
treeTest_with_results %>%
  mutate(outcome = case_when(prediction == 1 & Event == 1 ~ "TP",
                             prediction == 1 & Event == 0 ~ "FP",
                             prediction == 0 & Event == 0 ~ "TN",
                             prediction == 0 & Event == 1 ~ "FN")) %>%
  group_by(Species, outcome) %>%
  summarize(
    count = n(),                              # Count the number of rows for each outcome
    .groups = "drop"
  ) %>%
  group_by(Species) %>%
  mutate(
    total = sum(count),                       # Total outcomes per species
    rate = count / total                      # Rate of each outcome within species
  ) %>%
  ggplot()+
  geom_bar(aes(x = outcome, y = rate), stat = "identity")+
  facet_wrap(~Species)+
  theme_bw()

```

### 3.4.6. ROC Curves

The ROC curve (or "receiver operating characteristic" curve) is a diagnostic to see how well our model is performing.

In the plot below - the y=x line represents a model that's a "coin flip" - by just guessing 1's and 0's you could make a model like this. If our ROC curve is convex and north of the line (like the one you see below) - it means that we have a fairly good model - we are doing better than a coin flip. If our model were a hard right angle shape - it would mean our model is "overfit" - it's predicting with extremely high accuracy, and probably is inflexible to new information.

```{r, warning = FALSE}
ggplot(treeTest_with_results, aes(d = as.numeric(Event), m = as.numeric(estimate))) + 
  geom_roc(n.cuts = 50, labels = FALSE) + 
  style_roc(theme = theme_grey) +
  geom_abline(slope = 1, intercept = 0, size = 1.5, color = 'grey')+
  theme_bw()

```

# 4. Feature engineering methods

The key to making a really good model is engineering independent variables that are strongly predictive of the dependent variable. In the examples above, the data sets we used were very clean, and had good predictive accuracy as-is. In real life, that's not usually the case. To make a good model, you need to use your intuition about the phenomenon you are modeling to form a hypothesis about what kind of information might be predictive of your outcome.

This might mean joining your data to a new data set (using a tabular or spatial join), recategorizing variables you already have in your set, or transforming them to conform to the assumptions of the model type in order get more predictive power.

## 4.1. Data

Let's use some data on 2023 Philadelphia home sales from the City of Philadelphia Office of Property Assessment to attempt to predict sales prices for single family homes. Our dependent variable will be `sale_price`, a continuous outcome.

The general model approach will be the "hedonic" model, where the price of a good is decomposed into it's constituent parts.

For homes, this means the price is a function of the internal characteristics (number of bedrooms, bathrooms, AC, garage, fixtures) the location (and the local amenities like schools), the condition, lot size, and comparable sales (nearby prices).

Let's load in some data - these are spatial data (geojson), which we can read using `st_read` from the `sf` package. Each house has a point geometry associated with it.

```{r, message=FALSE, warning = FALSE, echo = FALSE, results = 'hide'}

homes <- st_read("~/GitHub/R_FAQ_For_Planners/modeling/data/homes.geojson") %>%
  st_as_sf() %>%
  st_transform(4326)

```
We can make a very basic hedonic model using bedrooms, bathrooms, year built, and total livable area to see how much variance our basic predictors explain in the dependent variable `sale_price`.

The R-Squared is about .51.

```{r}

summary(lm(data = homes, sale_price ~ year_built + number_of_bedrooms + number_of_bathrooms + total_livable_area))

```


## 4.2. Reclassification

I have a hypothesis that the relationship between a house's age and it's sale price is not continuous. A new house might be very desirable, a post-war house might be in need of major renovations, and a house built in the 1700s or 1800s might have historic appeal. Take a look at the scatterplot below:

```{r}
ggplot(homes)+
  geom_point(aes(x = year_built, y = sale_price))+
  theme_bw()

```

Let's reclassify homes into a few different categories - `new`, `victorian`, `mid-century`, `modern`, and `historic`.

```{r}

homes <- homes %>%
  mutate(age_categorical = case_when(year_built <= 1880 ~ "historic",
                                     year_built > 1880 & year_built <= 1920 ~ "victorian",
                                     year_built > 1920 & year_built <= 1980 ~ "mid-century",
                                     year_built < 1980 & year_built <= 2000 ~ "modern",
                                     year_built > 2000 ~ "modern"))

```

Let's also specify that number of bedrooms are different categories - not numerical variables. A 6 bedroom house isn't inherently 3x more valuable than a 2 bedroom house - they are totally different types of homes!

```{r}
homes <- homes %>%
  mutate(bedrooms_categorical = as.factor(number_of_bedrooms))

```

How does this affect our model? It improves it a little bit!

```{r}
summary(lm(data = homes, sale_price ~ bedrooms_categorical + number_of_bathrooms + total_livable_area + age_categorical))

```

## 4.3. Spatial Variables

What's the most important thing about a house? Location, Location, Location - that's what they always say!

Let's create variables that account for some things about a house's location.

### 4.3.1. Spatial Fixed Effects

We can join our homes to the zip codes to which they belong. This creates a proxy variable for "neighborhood" that you can use as a fixed effect in a model.

Let's load a shape of Philadelphia zip codes and make sure it's projected in crs = 4326 (same as our homes) by using `st_transform`.

```{r, message = FALSE, warning = FALSE, echo = FALSE, results = 'hide'}

zips <- st_read("~/GitHub/R_FAQ_For_Planners/modeling/data/zctas_philly.geojson") %>%
  st_as_sf() %>%
  st_transform(4326)

```
Now we can do a point-in-polygon join, adding our zip code information to our homes.

```{r}
homes <- st_join(homes, zips)
```

We can code zip code as a categorical variable - each "neighborhood" represents a different category of location.

```{r}

homes <- homes %>%
  mutate(zip_code = as.factor(zip_code))

```


If we run another model that includes the `zip_code` fixed effect, we can see that there is an enormous boost in the R-squared value.


```{r}
summary(lm(data = homes, sale_price ~ bedrooms_categorical + number_of_bathrooms + total_livable_area + age_categorical + zip_code))

```

### 4.3.2. K-Nearest-Neighbor Distance and Value

You might want engineer variables that account for the distance of a house from the nearest commercial corridor, or the average price per square foot of the 3 nearest sales. Using the sf package, this is all possible.

Using the `FNN` package, you could calculate the nearest neighbor distance from each home to the nearest 3 restaurants or parks.

Using the `spdep` package, you could calculate the average square footage price of the 3 nearest sales - a good parameter to represent "comps" - nearby sales (demonstrated below).

```{r, message = FALSE, warning = FALSE, echo = FALSE}
library(spdep)


neighborList <- knn2nb(knearneigh(st_coordinates(homes), 5))

spatialWeights <- nb2listw(neighborList, style="W")

homes$lagPriceSqFt <- lag.listw(spatialWeights, homes$sale_price/homes$total_livable_area)

```


```{r}
summary(lm(data = homes, sale_price ~ bedrooms_categorical + number_of_bathrooms + total_livable_area + age_categorical + zip_code + lagPriceSqFt))

```
## 4.4 Variable Transformations and Interactions

One of the core assumptions of regression modeling is that variables be normally distributed. With predictive modeling, we sometimes play fast and loose with assumptions, but the models do perform better when this is the case.

Here are a few ways you can transform your variables.

**Normalization** - you can use rate statistics or ratios as your modeling yardsticks. Perhaps it's more accurate to estimate price-per-square-foot than raw price.

**Standardization** - you can recenter your variables in a normal distribution centered on zero. Linear regression functions better with normally distributed variables.

**Transformation** - You can take variables that aren't normally distributed and transform them using log transformation. This is done very often with pricing models, where home prices are logged before modeling.

A last engineering technique is to "interact" variables.

**Interactions** - When the relationship between an independent predictor and the dependent variable depend on another predictor variable, you can *interact* the two variables in the model using the `*` multiply sign.


# 5. Data Dictionaries

## 5.1. Crop Yield Dataset

    - 


## 5.2. Wood Et Al


    - No: Seedling unique ID number.
    - Plot: Number of the field plot the seedling was planted in. (1-18)
    - Subplot: Subplot within the main plot the seedling was planted in. Broken into 5 subplots (1 per corner, plus 1 in the middle). (A-E)
    - Species: Includes Acer saccharum, Prunus serotina, Quercus alba, and Quercus rubra.
    - Light ISF: Light level quantified with HemiView software. Represents the amount of light reaching each subplot at a height of 1m.
    - Light Cat: Categorical light level created by splitting the range of Light_ISF values into three bins (low, med, high).
    - Core: Year the soil core was removed from the field.
    - Soil: Species from which the soil core was taken. Includes all species, plus Acer rubrum, Populus grandidentata, and a sterilized conspecific for each species.
    - Adult: Individual tree that soil was taken from. Up to 6 adults per species. Used as a random effect in analyses.
    - Sterile: Whether the soil was sterilized or not.
    - Conspecific: Whether the soil was conspecific, heterospecific, or sterilized conspecific.
    - Myco: Mycorrhizal type of the seedling species (AMF or EMF).
    - SoilMyco: Mycorrhizal type of the species culturing the soil (AMF or EMF).
    - PlantDate: The date that seedlings were planted in the field pots.
    - AMF: Percent arbuscular mycorrhizal fungi colonization on the fine roots of harvested seedlings.
    - EMF: Percent ectomycorrhizal fungi colonization on the root tips of harvested seedlings.
    - Phenolics: Calculated as nmol Gallic acid equivalents per mg dry extract (see manuscript for detailed methods)
    - NSC: Calculated as percent dry mass nonstructural carbohydrates (see manuscript for detailed methods)
    - Lignin: Calculated as percent dry mass lignin (see manuscript for detailed methods)
    - Census: The census number at which time the seedling died or was harvested.
    - Time: The number of days at which time the seedling died or was harvested.
    - Event: Used for survival analysis to indicate status of each individual seedling at a given time (above)
        0 = harvested or experiment ended
        1 = dead
    - Harvest: Indicates whether the seedling was harvested for trait measurement.
    - Alive: Indicates if the seedling was alive at the end of the second growing season. "X" in this field indicates alive status.
