---
title: "Introduction to Applied Predictive Modeling"
author: "Michael Fichman"
date: "2025-01-30"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    code_download: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# 1. Introduction

The world is awash in complex predictive tools and analytics. Some of these tools are very useful for planners and others concerned with social and spatial phenomena. The best way to develop utility with these kinds of tools is by starting with the basics - regression models. These models are interpretable, simple, and often deceptively powerful.

This is a beginner's guide to predictive analytics. It introduces a few simple model workflows and basic concepts to users who have little or no background working with statistics and/or computer coding. It is NOT a comprehensive text on regression models. Such texts are many in number, but if you want to get into one - I quite like [Regression and other Stories, by Gelman et al](https://avehtari.github.io/ROS-Examples/examples.html#Introduction).

Each section in this document details a different regression model we will use this semester in Land Use and Environmental Modeling (CPLN 6750 and the University of Pennsylvania Weitzman School of Design), and provides simple workflows in R using sample data sets.

The learning objectives are as follows:

1. Understand what it means to "model" data using regression.

2. Understand how regression models can be used to estimate outcomes, and learn how model selection is related to the outcome of interest.

3. Learn about the training and testing workflow used to validate models in R.

4. Learn basic regression terminology and statistics associated with OLS and binomial logit regression.

4. Understand the notion of model "accuracy" and "generalizability"

5. Learn how feature engineering can be used to improve model quality.

## 1.1. Explanation versus Prediction

You very often see regression models used in scholarly papers to show the "effect of A on B" or the "association between A and B"... "all else equal." In a public policy or planning context, causal and associative applications of regression are very useful for measuring policy effects or understanding social scientific relationships in a statistical framework. This is NOT what we are going to do. You will not hear any "A causes B" in this text.  Furthermore, we are not trying to use controls to isolate the associations between A and B, "all else equal." What this means is that we are going to violate some assumptions (e.g. the "rules") that will make econometric users of regression absolutely cringe.

We are going to use regression models for a different purpose - to estimate outcomes (dependent variables) given a bunch of predictors (independent variables). To give you an example of the difference in approach, if I want to estimate the heights of a number of people, I could do so very accurately if I fed a regression model information about their wingspans. However, that can't tell me anything about WHY a person is so tall.

The key to making above average predictive models is finding good independent variables and harnessing the power of their correlation with the dependent variable to make predictions. They key to making great predictive models is "engineering" good "features" to make models that are both accurate and flexible.

## 1.2. Model Types

We are going to demonstrate the use of two types of models, each is designed to suit a different type of outcome variable. First, we will use Ordinary Least Squares (OLS) regression to estimate normally distributed, continuous outcomes. Second we will use binary logistic regression to estimate 0/1 outcomes (e.g. estimating the probability of an outcome being a 1). Lastly we will use the Poisson regression model to estimate count data.

Your choice of model depends on the use case.

## 1.3. How Do Predictive Models "Work?"

All of these models are built around a simple framework - making a sort of "best fit" line through your data. You've certainly seen an x-y scatterplot with a "best fit" line plotted through the middle. This is actually a simple regression model - an OLS model fits a function that predicts the value of Y for a given X. Functionally, it minimizes the distance between the observations (the error) and the best fit line.

```{r echo=FALSE, message=FALSE, include=FALSE}

library(tidyverse)

# Set seed for reproducibility
set.seed(123)

# Generate random x-y data
data <- tibble(
  x = rnorm(100, mean = 50, sd = 10),  # 100 random x values from normal distribution
  y = 2 * x + rnorm(100, mean = 0, sd = 15)  # y follows a linear trend with some noise
)

# Plot the data with a best-fit line
ggplot(data, aes(x = x, y = y)) +
  geom_point(color = "blue", alpha = 0.6) +  # Scatter plot points
  geom_smooth(method = "lm", color = "red", se = TRUE) +  # Best-fit line with confidence interval
  theme_minimal() +
  labs(title = "Randomly Generated X-Y Data with Best Fit Line",
       x = "X Values",
       y = "Y Values")

```

Each of the models in this text apply a different fitted function to your data, but in the end, the predictive functionality is similar - they all output the "average" estimate given all the independent variables in the model.

## 1.4 Validating Models

How do you know if your predictive model is doing a good job? We can split our data. We "train" our model using a portion of our data, and hold out a "test" set with all the same characteristics. When we "show" our model the independent variables from our test set, we can make predictions

We will have a few ways to judge that:

- Is the model accurate on unseen data? Are the predictions close to the known values in our test set?

- Does the model performance generalize across contexts? Do we have big errors with some particular category of observation and small ones with another?

The "error metrics" vary depending on what type of model you are using, as we will see.

## 1.5. R Packages

We will use a few basic packages in this text.

If you do not have any of the packages listed below installed on your computer, install them like so:

```{r install_pkg, eval=FALSE, include=TRUE}
install.packages('tidyverse')
```

If you have already installed a package on your computer, you can load it into your environment using the `library` command.

```{r load_pkg , message = FALSE, warning = FALSE}
library(tidyverse)
library(sf)
library(caret)
library(readr)
```


# 2. Ordinary Least Squares Linear Regression

OLS regression, or multiple linear regression, is a simple model that can estimate the value of a continuous outcome - like prices or quantities. It's not good for count data, like numbers of incidents. It is simple, interpretable, and widespread.


## 2.1. Data set

We are going to use a [data set from the World Bank](https://www.kaggle.com/datasets/patelris/crop-yield-prediction-dataset?select=yield.csv) to see if we can predict national crop yields. This data set consists of agricultural yields for many types of crops in over 100 countries over a series of years. There is accompanying info about yearly rainfall and temperature. I have cleaned these data a bit to consist only of a few years. 

Let's say we are interested in food system planning, and we want to be able to predict the yield (weight per hectre) for several crop types - maize, wheat, etc., under several scenarios for the coming year related to rainfall and temperature - a hot year, a rainy year, etc.,

```{r load data}
yield_df <- read_csv("~/GitHub/R_FAQ_For_Planners/modeling/data/yield_df.csv") %>%
  rename(yield_hg_per_ha = `hg/ha_yield`) %>%
  group_by(Area) %>% 
  arrange(Item, Year) %>% 
  group_by(Area, Item) %>% 
  mutate(yield_last_year = lag(yield_hg_per_ha)) %>%
  ungroup() %>%
  filter(Year > 2010) %>%
  group_by(Area, Item, Year) %>%
  slice(1) %>%
  ungroup() %>%
  filter(is.na(yield_last_year) == FALSE)
```

## 2.2. Exploratory Analysis

The first step is always to do a few quick pieces of exploratory data analysis - looking at the data types in your set, the distributions of the data, and the correlations between different variables.

### 2.2.1. Data types

The `glimpse` command from `dplyr` in the `tidyverse` is the best way to quickly get a look at your data. Here we see that we have rows that have a column id, a country, a crop type, a year, yield per hectare (our continuous dependent variable - `yield_hg_per_ha`)

```{r}
glimpse(yield_df)
```
### 2.2.2. Distributions

We can look at how our continuous variables are distributed. Some of them are similar - very right-tailed (including our dependent variable). Others are a bit all over the place.

```{r}

yield_df %>%
  select(yield_hg_per_ha, average_rain_fall_mm_per_year, yield_last_year, pesticides_tonnes, avg_temp) %>%
  gather(key = "variable", value = "value") %>%
  ggplot()+
  geom_histogram(aes(value))+
  facet_wrap(~variable, scales = "free")+
  theme_minimal()


```


### 2.2.3. Correlations - Continuous Variables

Good predictive modeling is all about harnessing the power of correlation between independent predictors and your dependent variable. Take a look at how your continuous variables might be correlated with your outcome variable `yield_hg_per_ha`

A few notable observations - it seems that there is little relationship between yield and rainfall, there are some major outliers in pesticide application amounts, and that a previous year's yield by crop and country has a very, very strong relationship to current year yield.

```{r}
yield_df %>%
  select(yield_hg_per_ha, average_rain_fall_mm_per_year, yield_last_year, pesticides_tonnes) %>%
  gather(-yield_hg_per_ha, key = "variable", value = "value") %>%
  ggplot()+
  geom_point(aes(x = value, y = yield_hg_per_ha))+
  facet_wrap(~variable, scales = "free")+
  theme_minimal()
```
### 2.2.4. Correlations - Categorical variables

Let's examine how our dependent variable might vary from category to category.

This is a bit messy, but we can see that from country to country, and crop to crop, there is a lot of variation in yield. We would want to know what crop and country we are predicting for to generate an accurate estimate.

```{r}
yield_df %>%
  select(yield_hg_per_ha, Area, Item) %>%
  gather(-yield_hg_per_ha, key = "variable", value = "value") %>%
  ggplot()+
  geom_bar(aes(x = value, y = yield_hg_per_ha), stat = "identity")+
  facet_wrap(~variable, scales = "free")+
  coord_flip()+
  theme_minimal()
```

## 2.3. A Simple Model

Here's a simple example so we can take a look at our modeling outputs. Le's start by predicting yield using only rain fall, temperature and the amount of pesticides. This model is called `reg1`. We create the model using the `lm` function, which takes a data set, and an equation with a dependent variable (`yield_hg_per_ha`), then you put the independent variables on the other side of a `~` thingy.

```{r}

reg1 <- lm(data = yield_df, yield_hg_per_ha ~ average_rain_fall_mm_per_year + avg_temp + pesticides_tonnes)
```

You can use the `summary` command to look at the model outputs.

The model summary shows us a lot of information. Let's just focus on a three items of information - Beta Coefficients, p-values, and R-Squared.

The beta coefficients aka `Estimate` - these show the strength (big or small) and the direction (positive or negative) of the relationship between an independent variable and our dependent variable (yield) given the other variables in the model. Here is an example of how they are interpreted using technical language. "On average, a one unit increase in `average_rain_fall_mm_per_year` is associated with a 5.335 unit increase in `yield_hg_per_ha`, all else equal."

The p-value `Pr(>|t|)` represents the probability that this estimate is significantly different from zero (e.g. how much confidence do we have in the coefficient). A low p-value means a relatively high confidence in the estimate. Here it is `0.0773` - this is very low, but not low enough to show significance of the relationship in the eyes of, say, a medical journal reviewer. For our purposes though, a 7% chance the estimate is invalid, that's means there's a good chance this is a useful relationship for prediction.

The R-Squared value tells us the amount of variance in the dependent variable that is explained by this combination of independent variables. This isn't the same as an "accurate" model, but higher values generally relate to more powerful predictive models. Here, the value is 0.02 - which is horrible.

```{r}
summary(reg1)

```

Do we care about the beta coefficients, the p-values and the R-squared? Only a little bit - but nowhere close to as much as most people do when they use regression. We want to know whether these variables in this model predict yields well - we don't care about explaining why.  We can use these as diagnostics as we are building our model - adding features and engineering new ones. The real metrics we care about are discussed later.

## 2.4. Fixed Effects

Let's add some more variables - the type of crop, and the country where it was grown. It would stand to reason that the model "knowing" what country you're predicting yields for would help. Some countries are probably easier than other to grow crops. Our exploratory analysis told us that yields vary a lot across crop and country.

When you add a categorical variable, like country (the variable `Area`), it's known as a "fixed effect" or a "dummy variable" - where each category is given it's own line item in the regression summary, complete with an estimate. One level of the fixed effect is not represented in the summary - so there's one country you don't see here. Each estimate and p-value relates to that "base case" hold out.

For example, in this regression - each coefficient represents the unit change in crop yield associated with being in Algeria relative to being in Albania (the hold out country), or the yield associated with Maize relative to Cassava.

Adding these fixed effects made our R-Squared go wayyyy up - if we know the rainfall, country, crop and pesticide application, we can explain almost 80% of the variance in crop yields. Pretty good - but not quite a prediction.

```{r}

reg2 <- lm(data = yield_df, yield_hg_per_ha ~ average_rain_fall_mm_per_year + avg_temp + pesticides_tonnes + Item + Area)

summary(reg2)

```

Let's try a different approach - we saw there is a very strong relationship between yield and `yield_last_year` - what if we took the country out of the model, and put this in.... wow. That's a very strong predictor and a very high R-squared.

You can see that it basically knocks rainfall, temperature and pesticides out of the model in terms of significance (p-values). This is because these things are "multi-colinear" - a sign that the estimates are not very reliable when these variables are in the model together. This is a major way that prediction and explanation are very different when you use OLS models - we are violating some important stats assumtions.

```{r}

reg3 <- lm(data = yield_df, yield_hg_per_ha ~ average_rain_fall_mm_per_year + 
                       avg_temp + pesticides_tonnes + Item + yield_last_year)

summary(reg3)

```


## 2.5. Training and Testing

OK, now let's set up a prediction workflow. We do this by cutting up our data into a "training" and "testing" set - we "train" the model on one set, and then use it to predict on the "test" set where we know the yields already. The difference between the observed and predicted yields is our error.

### 2.5.1. Splitting our Data

We use the `set.seed` command to generate a random number sequence. We then use the `createDataPartition` command from the `caret` package to split our data. The `p` argument takes a number - here we say `.70` to indicate that we want 70% of our data in the training set. There's no set rule for the split size. The `paste` command is where we specify categorical variables - we want to make sure that single observations of categories end up in our test set. Why? If we train a model and it hasn't "seen" one of our countries, and then we "show" it that country in the test set, it won't be able to make a prediction for it.

We create two data frames - `yield_training` and `yield_test`

```{r}
set.seed(1234)

inTrain <- createDataPartition(
              y = paste(yield_df$Area), 
              p = .70, list = FALSE)


yield_training <- yield_df[inTrain,] 
yield_test <- yield_df[-inTrain,]  
```

### 2.5.2. Creating a Regression

Let's put our independent variables from `reg3` - the one with the year lag - into an `lm` object using `yield_training`, our training set. We'll name this `reg.training_1`.

```{r}
reg.training_1 <- 
  lm(data = yield_training, 
     yield_hg_per_ha ~ average_rain_fall_mm_per_year + 
                       avg_temp + pesticides_tonnes + Item + yield_last_year)

```

### 2.5.3. Predicting for our Test Set

We can now use the `predict` function to "show" our test set `yield_test` to the model `reg.training_1`, and make a prediction for each row.

We want to know how much our predictions missed the mark - in absolute and percentage terms.

Here we create a new data frame called `yield_test_with_results`, we take `yield_test` and mutate some new columns - our `Prediction`, an `Error` that equals prediction minus actual, and measures of Absolute Error (`AbsError`) and Absolute Percent Error (`Abs_Pct_Error`) for each observation in the test set.

```{r}
yield_test_with_results <-
  yield_test %>%
  mutate(Prediction = predict(reg.training_1, yield_test),
         Error = Prediction - yield_hg_per_ha,
         AbsError = abs(Prediction - yield_hg_per_ha),
         Abs_Pct_Error = (abs(Prediction - yield_hg_per_ha)) / yield_hg_per_ha)
```

Last step, we can summarize our errors - what is our Mean Absolute Percentage Error (MAPE)?

What is our Mean Absolute Error (MAE)?

Not bad!

On average, we can predict yearly crop yields within 13%.


```{r}
yield_test_with_results %>%
  summarize(MAPE = mean(Abs_Pct_Error),
         MAE = mean(AbsError))

```
The last thing we want to do is see if our model is performing better or worse depending on the context. It would be good to know if we are making accurate predictions about, say, soybeans, but missing badly on maize.

We can group our results by `Item` and summarize our errors by crop type. As you can see, there is some variability,.

```{r}
yield_test_with_results %>%
  group_by(Item) %>%
  summarize(MAPE = mean(Abs_Pct_Error),
         MAE = mean(AbsError))

```

# 3. Binary Logistic Regression for Classification

Binary logistic regression, or "logit" is different from OLS modeling because it is designed to estimate binary outcomes. Specifically, it estimates the probability of a combination of variables you stick in a model predicts a "1" or a "0".

It does this by fitting an s-shaped function to 0/1 data, unlike OLS, which fits a linear function to continuous data.

The tricky part is that you end up with predictions that range from 0-1, and you need to figure out what threshold to use to classify your predictions, which may be values like 0.7 or 0.35, as "1" or "0".


```{r}


# Generate predictor variable (x)
ex_data <- tibble(
  x = rnorm(100, mean = 0, sd = 2)  # 100 random values from N(0,2)
)

# Define a logit function with coefficients
beta_0 <- -1  # Intercept
beta_1 <- 2   # Slope

# Compute probabilities using logistic (sigmoid) function
ex_data <- ex_data %>%
  mutate(prob = exp(beta_0 + beta_1 * x) / (1 + exp(beta_0 + beta_1 * x)),
         y = rbinom(100, size = 1, prob = prob))  # Convert probabilities to binary outcome

# Fit a logistic regression model
logit_model <- glm(y ~ x, data = ex_data, family = binomial)

# Plot data with logistic regression fit
ggplot(ex_data, aes(x = x, y = y)) +
  geom_jitter(width = 0.1, height = 0.05, alpha = 0.5, color = "blue") +  # Jitter to avoid overplotting
  geom_smooth(method = "glm", method.args = list(family = binomial), color = "red") +  # Logistic fit
  theme_minimal() +
  labs(title = "Simulated Logit Function with Logistic Regression Fit",
       x = "X values",
       y = "Binary Outcome (Y)")

```

## 3.1. Data set

This is a data set about tree survival published [by the aptly named Wood et al](https://datadryad.org/stash/dataset/doi:10.5061/dryad.xd2547dpw) at Michigan State. These data consist of ~2700 observations of trees, whether they are dead or alive, and a list of traits of that tree and the immediate environment.

The "use case" for our modeling example will be a forester who is interested in managing land for fire load and looking to target trees that they estimate are likely to be dead for removal and sale as timber product.

```{r, message = FALSE, warning = FALSE}
trees <- read_csv ("~/GitHub/R_FAQ_For_Planners/modeling/data/tree_morbidity.csv") %>%
  filter(is.na(Event)== FALSE) %>%
  mutate(Species = as.factor(Species),
         Event = as.factor(Event))

```

## 3.2. Exploratory Analysis

Let's take a look at our data set - our dependent variable is called `Event` - and it is binary. It's 1 if the tree is dead, and 0 if it's not.

We will take a similar approach as we did with OLS regression, examining variables in relation to one another and to the dependent variable.

## 3.2.1. Looking at our data set

```{r}
glimpse(trees)

```

## 3.2.2. Comparing continuous predictors across levels

We can use some simple `dplyr` logic to look at whether the mean values of our continuous variables tend to be different depending on whether our `Event` binary outcome is 1 or 0.

At first glance, it looks like lower concentration of lignin and phenolics in the organism is associated with death, as is time (which is time from planting to harvesting).

```{r}
trees %>%
  select(Event, Time, NSC, Lignin, Phenolics, EMF, AMF) %>%
  gather(-Event, key = "variable", value = "value") %>%
  group_by(variable, Event) %>%
  summarize(mean = mean(value, na.rm = TRUE)) %>%
ggplot()+
  geom_bar(aes(y = mean, x = Event), stat = "identity")+
  facet_wrap(~variable, scales = "free")+
    theme_bw()

```

Another approach worth trying is looking at the distributions of the different variables by making histograms that are split by 0/1 and by variable.

Using this approach I can see that amongst trees that die, lignin and phenolics and NSC seem to have right tailed distributions, and there's a discrete split at a certain value. We can also see that something seems kind of weird with the "Time" variable - maybe it represents two harvest periods.

```{r}
trees %>% 
    select(Event, Time, NSC, Lignin, Phenolics, EMF, AMF) %>%
    gather(-Event, key = "variable", value = "value") %>%
    ggplot()+
    geom_histogram(aes(value))+
    facet_grid(Event~variable, scales = "free")+
    theme_bw()

```


## 3.2.3. Comparing categorical predictors across levels

We can use some simple `dplyr` logic to look at whether our categorical variables vary depending on whether our `Event` binary outcome is 1 or 0.

It seems like species has a big effect - huge differences between the species in the frequency of 0's and 1's.


```{r}
trees %>%
  select(Event, Soil, Sterile, Conspecific, Myco, SoilMyco, Species) %>%
  gather(-Event, key = "variable", value = "value") %>%
  count(variable, value, Event) %>%
  ggplot()+
  geom_bar(aes(value, n, fill = Event), 
           position = "dodge", stat="identity") +
  facet_wrap(~variable, scales="free") +
  coord_flip()+
    theme_bw()

```

## 3.3. A Simple Model

You can play around with lots of different combinations in the model

Soil types and Mycorrhizal types seem to be very collinear, so I've left them out of the model...

Feature engineering thing - based on exploratory analysis, cut lignin etc. up a categorical variables.

mutate(Lignin = ifelse(Lignin > 15, "High", "Low"))

Logit function
Coefficients
McFadden
p-values

```{r}

reg_logit <- glm(data = trees , 
                 Event ~ Light_ISF + NSC + Lignin + Phenolics + Species + Soil,
                 family="binomial")

summary(reg_logit)

```

Interpret the coefficients

The odds of a _____ dying are X times that of a _______

All else equal, the odds of a tree dying increase by x% (e.g. 1.14 is 14%) with a unit change in the independent variable.

```{r}
exp_coef <- reg_logit$coefficients %>%
  as.data.frame() %>%
  rename(coefficient = ".") %>%
  mutate(exponentiated = exp(coefficient))

```


## 3.4. Training and Testing

```{r}
set.seed(1234)

treetrainIndex <- createDataPartition(y = paste(trees$Species, trees$Soil), 
                                  p = .70,
                                  list = FALSE,
                                  times = 1)

treeTrain <- trees[ treetrainIndex,]
treeTest  <- trees[-treetrainIndex,]

```

```{r}
logit_train <- glm(data = treeTrain, Event ~ Light_ISF + NSC + Lignin + Phenolics +  
                                       Species + Soil,
                 family="binomial"(link="logit"))
```

```{r}

treeTest_with_results <-
  treeTest %>%
  mutate(estimate = predict(logit_train, .,  type="response"))

```

Let's talk about how to interpret this and what the threshold should be.

```{r}
treeTest_with_results %>%
  ggplot()+
  geom_histogram(aes(estimate))+
  facet_wrap(~Event, nrow= 2)+
  theme_bw()

```
### 3.4.2. Confusion Metrics

Let's create the results again but let's add the threshold to classify 1/0

```{r}

treeTest_with_results <-
  treeTest %>%
  mutate(estimate = predict(logit_train, .,  type="response"),
         prediction = as.factor(ifelse(estimate >= 0.4, 1, 0)))
  

```

True Positive
True Negative
False Positive
False Negative

Accuracy

Sensitivity
Specificity

```{r}
caret::confusionMatrix(treeTest_with_results$prediction, as.factor(treeTest_with_results$Event), positive = "1")

```

You can make your own version of this using `mutate` commands, and then assess model accuracy across categories.

If we look at our model performance across species, we see that our model tends to predict negatives (eg 0) for our Quercus species - be those true or false negatives. 

```{r}
treeTest_with_results %>%
  mutate(outcome = case_when(prediction == 1 & Event == 1 ~ "TP",
                             prediction == 1 & Event == 0 ~ "FP",
                             prediction == 0 & Event == 0 ~ "TN",
                             prediction == 0 & Event == 1 ~ "FN")) %>%
  group_by(Species, outcome) %>%
  summarize(
    count = n(),                              # Count the number of rows for each outcome
    .groups = "drop"
  ) %>%
  group_by(Species) %>%
  mutate(
    total = sum(count),                       # Total outcomes per species
    rate = count / total                      # Rate of each outcome within species
  ) %>%
  ggplot()+
  geom_bar(aes(x = outcome, y = rate), stat = "identity")+
  facet_wrap(~Species)+
  theme_bw()

```

ROC & AUC

# 4. Feature engineering methods

The key to making a really good model is engineering independent variables that are stronly predictive of the dependent variable. This might mean joining your data to a new data set, recategorizing variables you already have in your set, or transforming them to get more predictive power.

Maybe do this with some housing data - spatial fixed effects, st_distance, reclassification, log transformation


Reclassification
Normalization
Transformation
knn
spatial join

# 5. Data Dictionaries

## 5.2. Wood Et Al


    - No: Seedling unique ID number.
    - Plot: Number of the field plot the seedling was planted in. (1-18)
    - Subplot: Subplot within the main plot the seedling was planted in. Broken into 5 subplots (1 per corner, plus 1 in the middle). (A-E)
    - Species: Includes Acer saccharum, Prunus serotina, Quercus alba, and Quercus rubra.
    - Light ISF: Light level quantified with HemiView software. Represents the amount of light reaching each subplot at a height of 1m.
    - Light Cat: Categorical light level created by splitting the range of Light_ISF values into three bins (low, med, high).
    - Core: Year the soil core was removed from the field.
    - Soil: Species from which the soil core was taken. Includes all species, plus Acer rubrum, Populus grandidentata, and a sterilized conspecific for each species.
    - Adult: Individual tree that soil was taken from. Up to 6 adults per species. Used as a random effect in analyses.
    - Sterile: Whether the soil was sterilized or not.
    - Conspecific: Whether the soil was conspecific, heterospecific, or sterilized conspecific.
    - Myco: Mycorrhizal type of the seedling species (AMF or EMF).
    - SoilMyco: Mycorrhizal type of the species culturing the soil (AMF or EMF).
    - PlantDate: The date that seedlings were planted in the field pots.
    - AMF: Percent arbuscular mycorrhizal fungi colonization on the fine roots of harvested seedlings.
    - EMF: Percent ectomycorrhizal fungi colonization on the root tips of harvested seedlings.
    - Phenolics: Calculated as nmol Gallic acid equivalents per mg dry extract (see manuscript for detailed methods)
    - NSC: Calculated as percent dry mass nonstructural carbohydrates (see manuscript for detailed methods)
    - Lignin: Calculated as percent dry mass lignin (see manuscript for detailed methods)
    - Census: The census number at which time the seedling died or was harvested.
    - Time: The number of days at which time the seedling died or was harvested.
    - Event: Used for survival analysis to indicate status of each individual seedling at a given time (above)
        0 = harvested or experiment ended
        1 = dead
    - Harvest: Indicates whether the seedling was harvested for trait measurement.
    - Alive: Indicates if the seedling was alive at the end of the second growing season. "X" in this field indicates alive status.
