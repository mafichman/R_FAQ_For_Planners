---
title: "Intro to R and FAQ for Planners"
author: "Michael Fichman"
date: "8/17/2022"
output: 
  html_document:
    toc: true
    toc_float: true
    code_download: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Introduction

This is a working document designed to give a basic introduction to R for those who have no coding experience. This should help you get prepared for applied projects in CPLN and MUSA 500-600 level coursework within a couple of weeks. It coalesces introductory materials I've developed over the past five years for MUSA 508, MUSA 795, CPLN 501, and CPLN 675 and should give you what you need to get started in any of those courses.

This isn't a "coding" tutorial, it's a starter kit that demonstrates how R can do some familiar Excel or ArcGIS-like tasks - loading spreadsheets and spatial data, creating new columns of data, summarizing and querying data, making charts and maps - and do them faster, better, and more reproducibly.

Use the table of contents at left to navigate through the document - the simplest topics are first (e.g. "What is R?", "How do I download R?"), the latter sections are more advanced (Geoprocessing, Census Data, Markdown documents). There are references to books and reference materials at the end of the doc.

This document will be updated on an ongoing basis (it's a construction site). Right now, sections 1-10 are complete - which is what students in CPLN 675 and MUSA 508 will need to get up and running this year!

**Michael Fichman**

*University of Pennsylvania Weitzman School of Design*

*Lecturer, Department of City and Regional Planning*

*Interim Program Director, Masters of Urban Spatial Analytics*

*Researcher, PennPraxis*

*Data Scientist, Center for Safe Mobility*

# 2. Getting Started with R

## What is R?

R is a programming language you can use to do stuff that you can do in Excel, or ArcGIS, or stats software like SAS... but (with some practice) faster, better, and more efficiently. 

You can load data in spreadsheet form, make new columns, create data summaries, make charts, make maps. You can load spatial data and use R as a GIS. You can make statistical models.  You can also do way more complicated stuff too. *Most importantly, you can set up routines or workflows to let you repeat processes that in other software would take lots of clicks and steps.*

It's "open source," which means it is written and maintained by a community of people like you and me. That also means that sometimes, things will go wrong and there isn't always a manual to consult.

If you've never programmed before, that's OK, you're in the right place.

## What is R Studio?

I run R code and do projects in R Studio, which is like a viewer for R. You can write code to load and manipulate data. You can see plots and data sets that you've loaded using R Studio. 

## Installing R and R Studio for Windows and Mac. 

1.	Install R: Go to [https://cran.r-project.org/bin/windows/base/](https://cran.r-project.org/bin/windows/base/)
2.	Click on your operating system (Windows, MAC, Linux) and follow directions to DL the most recent version of R – which is 4.1.1 as of this writing.
3.	DL and run the .exe file to install
4.	Install R Studio: Go to [http://www.rstudio.com](http://www.rstudio.com), click on “Download RStudio” and follow the directions for your operating system. 
5.	Open R studio by clicking on the icon, R will run inside the R Studio user interface.

## R Studio Orientation

Now open up R Studio and let's take a look at some of the components.

![Image by Kieran Healy](images/healy_gui.png)
Your R Studio GUI (graphical user interface) should look something like this (Image by Kieran Healy)

### The Console

You can type and execute code in the console.

I could type something like `myName <- "Michael"` into the console, and it would create a variable called `myName` that is equal to the character string "Michael" in my "environment."

Go ahead, try it.

```{r michael}

myName <- "Michael"

```

### The Environment

This is where you can see the data sets and other things you've loaded into R to work on. If I load an excel spreadsheet into R using `read_excel`, I could say something like `myData <- read_excel("filepath_on_my_computer.xlsx")` it will show up in my environment as a thing called `myData`.

You can clear the environment by going to the `Session` dropdown and saying `Clear Workspace`.

You can also save your environment or load up an old one from a project you were working on.

### The Code window

If you load up a .R or .rmd file - two types of R files, they will show up as tabs in your code window.

If I give you some code, you can execute it a piece at a time by copying-and-pasting into the console, or highlighting code and hitting `ctrl+Enter` to run it.

If you are writing code, you can save your code as either a .R file (just plain code), or a .rmd file (a Markdown file, like what you are reading now, which is something you can write to present code or analysis.)

### The Plotting / Files / Packages etc Window

-If you make charts or plots - they show up in this window.

-The "Files" tab is a place to graphically look through file trees on your computer.

-You can consult the "Packages" or "Help" tabs to find documentation about different functions and packages in the R world. (More about packages later)

# 3. Data and Coding Basics

How do you "run code" in R?

Usually, I will give you a bunch of code in class, and you will have to adapt that code for a project you are doing. Eventually, you will write some of your own code - but *most people cut and paste stuff* and switch variable names and input data sets around. That's how I do much of my work - I grab things from my code base I've generated over the years. So don't worry about writing things from scratch.

- Code with a `#` in front of it is called "commented code." This stuff doesn't execute it's where you make notes.

- Code without the `#` will run - either as written in the console or by hitting ctrl+enter on code the code window

```{r commented}

# This code doesn't do anything, but the next line does
print('hello world')

```

R is case sensitive - many of your issues can be resolved by checking your syntax. You might get some kind of error because you created a data set called `derp` and you try to call it in the code with `DERP`. Happens *all the time*.

TUTORIAL:

**If this is your first time using R, you might want to try out a .R script called `running_code_in_R.R` which [you can find here](https://github.com/mafichman/R_FAQ_For_Planners/blob/main/R/running_code_in_R.R). It has some simple code and simple operations you can try - download the .R file, load it into your R Studio and follow the instructions to create some simple objects.**

## Naming Variables and Data Sets

Most pieces of data you load into R or create with R will have a name. If you load in a spreadsheet, you will want to give it a name so it shows up in my R environment.  You can assign information to a variable using this sign "<-"

You can have all kinds of data objects:

- A data frame: a matrix of data with column names, basically like an excel spreadsheet (much more on this later)

- A vector: a list of items, denoted with the letter c, like this:
```{r myVector}
myVector <- c("A", "B", "C")
```
- A character: some things treated as text, denoted with "" quotes around it.

```{r myVariable}
myVariable <- "derp"
```
- A number: can be in the form of an integer or a double (a floating point number).

```{r myNumber}
myNumber <- 10
```

## NA Data

There are numbers, characters, and other data types. There are also `NA` data - missing data - these are tricky. You can't do arithmetic on these.

Look at the example below. We create a vector of numeric data called `vector1` which is just numbers, and then we ask its median. That works.

Then we create one called `vector2` with some NA in there, it doesn't work to take the median there.

```{r vector1}
vector1 <- c(1, 4, 6, 7, 10)

median(vector1)
```

```{r vector2}
vector2 <- c(1, 4, 6, NA, 10)

median(vector2)
```

# 4. Installing and Loading Packages

A lot of the things you will want to do with R will require loading packages. "Base R" is basic R - there are a bunch of functions it does. Since R is open source, lots of people have developed their own cool packages that do additional stuff.

A package is a set of functions - some of them make graphics, some handle spatial data. You will almost always start your session by loading some packages.

The first time you use a package on your computer, you can load it with the command `install.packages`, like this:

```{r install_pkg, eval=FALSE, include=TRUE}
install.packages('tidyverse')
```

If you have already installed a package on your computer, you can load it into your environment using the `library` command.

```{r load_pkg , message = FALSE, warning = FALSE}
library(tidyverse)
```

Packages all come with documentation - lists of functions in these packages have little vignettes that tell you how to use them.  Check that out in the "Packages" tab in your R Studio environment to see these, or just google package names or function names to find the documentation.

You can also type things like `??select` in your console and documentation about the function `select` will pop up in your "Help" window in R Studio.

## The Tidyverse

The `tidyverse`, which we loaded in the previous code chunk, is a group of important packages which are used to wrangle data, make graphics, load data, handle dates, handle text. Better yet, these projects are coordinated by their developers, so they all work nicely together.

For work in our class, you should load up the tidyverse on every project. Most of my code doesn't work without it.

# 5. Loading and Viewing Data 

Most projects will start with you loading some spreadsheet-like data - from a survey you conducted, from an excel sheet, from the Census' website, from a shapefile that I gave you for class. For non-spatial data, these spreadsheet-like data will be in a format called a "data frame" (or as a variant called a "tibble).

There are a lot of file types you can load, and some of them have their own functions, like you can read in Excel data with the `read_excel` function in the `readxl` package. Later on I will show you how to read in a .shp. (You can read more about these data import functionalities [here.](https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-import.pdf))

There are several ways to load data into your R environment.

## Loading Data From Your Computer

You can load data from your computer in one of two ways, either by going to `File / Import Data Set` in a dropdown menu R Studio, and clicking on the relevant options (note that "From txt" will let you load a csv, txt, tsv or other similar file types), or by doing it with code, like this:

```{r myData_cpu, eval = FALSE}
myData <- read.csv("filepath/onMyComputer/fileName.csv")
```

## Loading Data from the Web

You can load data from the web, like from Google Drive or Github, using the same kind of syntax. Here we load a csv from the web:

```{r myData_url}
myData <- read.csv("https://raw.githubusercontent.com/mafichman/R_FAQ_For_Planners/main/data/someCensusData.csv")
```

## Viewing Data

Now that you've loaded in some data, you can examine them in several ways. You can `View(myData)` and see a visual representation of the data in an excel-like way, or you can `glimpse` your data frame.

`glimpse` will tell you what data types the columns are (character, numeric, etc), let you see some samples of the data, and let you see how many rows and columns the data contain.

```{r glimpse_myData}
glimpse(myData)
```
## Writing Data to File

After you manipulate data, you might want to write it out to file. Everything we are doing in the R-Studio environment does not affect the underlying data - so any project you do might need a step where you out put data.

You can do this with commands like `write.csv` - so you could data your data frame `myData` and write it out as a file with a name and location of your choosing like so:

`write.csv(myData, "your_file_path/your_file_name.csv")`

We talk a bit later about reading/writing spatial data as well.

# 6. Manipulating Data with dplyr

`dplyr` is the core data wrangling package in "tidy" R. You can use functions from `dplyr` to manipulate data frames. 

You can make new columns, and remove or rename columns from your data frame. You can summarize data (e.g. what is the sum of all the rows in a given column). You can filter data to keep only rows that meet some criteria.

It does lots of other stuff too.

There is a handy guide for all the dplyr data tools at [R Studio's dplyr cheat sheet](https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-transformation.pdf).

## Using the "Pipe"

*Very important*, to what I'm about to show you from here on out is a thingy called a "pipe" - `%>%` - which is an object you put in your code to chain functions together and make kind of data-wrangling recipes.

To understand what the pipe does, think about the following "recipe" like this: I make an `omlette` by taking a data frame called `eggs` and applying the functions `scramble` and `fry`

`omlette <- eggs %>% scramble() %>% fry()`

OK, keep that in mind while we look at `dplyr` commands.

## Dplyr commands

Let's use a few basic `dplyr` commands to wrangle our dataframe `myData`.

### Mutate

`mutate` makes a new column. I am going to *overwrite* `myData` - making a new version that has a new column in it. Each row of the data frame represents a census tract, and I'd like to see what the difference is in tract population between 2010 and 2016. I'll call this new column `pop_change` and set its value equal to `total_pop.2016` minus `total_pop.2010`. Note the pipe used to "chain" the mutate statement in there.

```{r mutate_first}

myData <- myData %>%
  mutate(pop_change = total_pop.2016 - total_pop.2010)

```

Did it work? Try typing `View(myData)` or `glimpse(myData)` into your console.

I'm now going to create a second variable called `pop_change_positive` using `mutate`. If the population change was positive, I'm going to call set this variable equal to `TRUE` - I'll do an `ifelse` statement to make it's value contingent on the value of `pop_change`.

```{r mutate_second}

myData <- myData %>%
  mutate(pop_change_positive = ifelse(pop_change > 0, TRUE, FALSE))

```

### Filter

`filter` lets you reduce your data frame based on some criteria. I'd like to create a new data frame called `population_loss` that consists only of census tracts in `myData` that lost population (e.g. pop_change_positive == FALSE).


```{r filter}
population_loss <- myData %>%
  filter(pop_change_positive == FALSE)
```

I can do this kind of filtering based on lots of criteria.

### Select

`select` keeps only certain columns that you want to keep.

I'm going to make a new data frame, called `twoVariables` that consists of only the columns `NAME.y` and `GEOID` from the data frame `myData`

```{r select}
twoVariables <- myData %>%
  select(NAME.y, GEOID)
```

### Rename

`rename` changes the names of your columns. I'd like to rename the column from the data frame `twoVariables` called `NAME.y` to be called just `NAME`

```{r rename}
twoVariables <- twoVariables %>%
  rename(NAME = NAME.y)
```

## Summarizing Data

You'll often want to know about the characteristics or central tendencies of your data. Using the `group_by` and `summarize` commands you can do lots of this.

Here are some examples - notice I'm not creating new data frames here.

First, let's just find out what the median `pop_change` was between 2010 and 2016 for Philadelphia census tracts.

```{r summarize_1}
myData %>%
  summarize(median_pop_change = median(pop_change))
```

What if we want to know how one type of tract versus another varied in terms of `pop_change`? We can group our data into categories using `group_by` and then summarize.

For tracts for which `pop_change_positive` (remember we created this column as a TRUE/FALSE - was there population change between 2010 and 2016?) what is the median `pop_change`?

```{r summarize_2}
myData %>%
  group_by(pop_change_positive) %>%
  summarize(median_pop_change = median(pop_change))
```

## Joining Data

You can do "tabular joins" between data sets when they have a "unique ID" - a number or code that identifies each observation or subject in your data. For example, the "GEOID" is the unique code associated with each census tract.

There are several types of joins you can use in R. Hadley Wickham's "R For Data Science" [has a great chapter and series of code examples](https://r4ds.had.co.nz/relational-data.html) on the mechanics of left, right, inner, outer, full, and other joins.

The basic mechanics of a join go like this:

`new_dataframe <- left_join(dataframe1, dataframe2, by = c("unique_ID"))`

This will get you a data set where you preserve all of `dataframe1` (the "left" side of the left join), and tack on anything from `dataframe2` that matches `unique_ID` found in `dataframe1`

Notice the `by` call where you specify the unique ID, that's a weird bit of syntax. You can join on multiple Unique IDs if you want.

## Wide Data and Long Data

One concept that is important but a bit hard to engage is data format. The dataframe `myData` that we have been working with so far is what's called a "wide" data set. Each row is a census tract, each column is a variable. This is great for doing column math with `mutate` and it's good for making certain kinds of plots - for instance a scatter plot with two continuous variables (more on this in the next section).

Take a look at `myData` using the command `View(myData)` and you'll see that it is indeed "wide" format.

There is another form of data called "long" data that are useful for other circumstances, like making comparisons between continuous variables across categories. (Note: Often you find time-series data in long format).

Let's examine indicators from 2010 of tracts that appreciated in population between 2010 and 2016:

- Median household income
- Median number of people identifying their racial background as "white alone"
- Median number of housing units

We `gather` those variables, and make a data frame where each row is a tract, it's `pop_change_positve` value (TRUE/FALSE), a variable name, and a value for that variable.

Then we can `group_by` the population change direction (TRUE - positive) and the `variable`, and summarize the `median_value` for each variable.

```{r gather_example}
myData %>% 
  select(pop_change_positive, med_HH_Income.2010,  total_White.2010, total_HU.2010) %>% 
  gather(-pop_change_positive, key = "variable", value = "value") %>% 
  group_by(pop_change_positive, variable) %>%
  summarize(median_value = median(value, na.rm = TRUE))

```

*Note* there is a more modern version of `gather` known as `pivot_longer`, where the syntax would look like this:

```{r pivot_longer, eval=FALSE}
myData %>% 
  select(pop_change_positive, med_HH_Income.2010,  total_White.2010, total_HU.2010) %>% 
  pivot_longer(cols = -pop_change_positive) %>%
  group_by(pop_change_positive, name) %>%
  summarize(median_value = median(value, na.rm = TRUE)) 
    
```

wide and long data are a weird concept - this graphic is pretty helpful understanding how data can transform from one form to another.

*Can you think of a few situations in which one form or another is appropriate?*

![Pivoting between wide and long data](https://raw.githubusercontent.com/mafichman/R_FAQ_ForPlanners/images/tidyr-spread-gather.gif)
Image: https://www.garrickadenbuie.com/

The wide/long data concept is a bit nebulous, but it really comes in handy when you are making facetted graphics - a subject covered in the next section.

# 7. Making Plots with ggplot2

`ggplot2` is the main graphics library in the `tidyverse` - it's fantastic. You can make bar plots, line plots, and much more with it.

You make ggplots by writing "recipes" for charts. The `+` operator is a bit like what the `%>%` is in dplyr/tidyverse - you use it to add elements together.

The core elements of ggplot recipes are called geometries (a `geom`), and the way you style the data is with aesthetics (`aes`). You can add some other styles to your ggplots, but if it involves changing something about the appearance of the plot based on data, you use an `aes` call.

There are lots of resources for ggplot - the simplest and easiest one to check out is [R Studio's ggplot cheat sheet](https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-visualization.pdf). There is also a whole (free) book on data viz in ggplot - [Data Visualization by Kieran Healy](https://socviz.co/). It's fantastic.

## Making a basic plot

Let's make a simple scatterplot.

Think about the "recipe" for the following chart where we see plot Philadelphia census tract median household income as a function of the tract's pencentage of vacant housing units using this narrative (remember each row in myData is a Philadelphia census tract):

- Make me a ggplot
- Add a point geometry from the data set `myData` where the x aesthetic is `vacancyPct.2016` and the y aesthetic is `med_HH_Income.2016`

```{r ggplot1, warning = FALSE}
ggplot()+
  geom_point(data = myData, 
             aes(x = vacancyPct.2016, y = med_HH_Income.2016))
```

## Adding more aesthetics

We can add more aesthetics to this plot. Let's do this first by just tinkering with the colors to make it look cool - we will do this with some commands *outside* the aesthetics to make the points blue.

```{r ggplot2, warning = FALSE}
ggplot()+
  geom_point(data = myData, 
             aes(x = vacancyPct.2016, y = med_HH_Income.2016),
             color = "blue")
```

OK, now let's make the point coloration change according to a data-related element, something we have to specify in the aesthetics. We specify the color as being equal to `pop_change_positive`:

```{r ggplot3, warning = FALSE}
ggplot()+
  geom_point(data = myData, 
             aes(x = vacancyPct.2016, y = med_HH_Income.2016, color = pop_change_positive))
```

## Facets

You can very simply make "small multiple" plots where you subdivide your data into categories for graphic comparison. This is done with the `facet_wrap` command. 

Here's an alternate take on our previous plot, cut into two plots. Notice in the code I pulled `pop_change_positive` out of the aesthetics from the previous plot and put it in the `facet_wrap` command.

```{r ggplot4, warning = FALSE}
ggplot()+
  geom_point(data = myData, 
             aes(x = vacancyPct.2016, y = med_HH_Income.2016))+
  facet_wrap(~pop_change_positive)
```

Remember how we used `gather` and `pivot_longer` to make our wide data into long data in the last section? That's an incredibly useful technique for making quick data-mining visualizations across a number of variables.

```{r ggplot_long, warning = FALSE, message = FALSE}
myData %>% 
  select(pop_change_positive, med_HH_Income.2010,  total_White.2010, total_HU.2010) %>% 
  pivot_longer(cols = -pop_change_positive) %>%
  group_by(pop_change_positive, name) %>%
  summarize(median_value = median(value, na.rm = TRUE)) %>%
ggplot()+
  geom_bar(aes(x = pop_change_positive, y = median_value), stat = "identity")+
  facet_wrap(~name, scales = "free")
```

## Advanced graphics

You can alter most elements of the ggplot. We could take our initial scatterplot and do some things to spruce it up, like add a title, change the axis labels etc.,

```{r ggplot5, warning = FALSE}
ggplot()+
  geom_point(data = myData, 
             aes(x = vacancyPct.2016, y = med_HH_Income.2016))+
  labs(title="Philadelphia Tract Med. HH Income as a Function of Pct. Vacancy",
       subtitle = "Data: US Census Bureau, 2016 ACS 5-Year Estimates",
        x ="Pct Vacant Housing Units (0-1)", y = "Median Household Income (2016 $)")
```

I like to make some pre-made recipes where I style the lines, fonts and other elements, and then I can just add it to my plots. Here's a recipe I call `plotTheme`. Load it up into your environment and then you can add it to your plots to make them nicer. The sky is the limit.

```{r plotTheme}

plotTheme <- theme(
  plot.title =element_text(size=12),
  plot.subtitle = element_text(size=8),
  plot.caption = element_text(size = 6),
  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
  axis.text.y = element_text(size = 10),
  axis.title.y = element_text(size = 10),
  # Set the entire chart region to blank
  panel.background=element_blank(),
  plot.background=element_blank(),
  #panel.border=element_rect(colour="#F0F0F0"),
  # Format the grid
  panel.grid.major=element_line(colour="#D0D0D0",size=.75),
  axis.ticks=element_blank())
```

Now I can add `plotTheme` to my recipe from the last plot for some extra styling. Notice I just tack it onto the recipe below with a `+`:

```{r ggplot6, warning = FALSE}
ggplot()+
  geom_point(data = myData, 
             aes(x = vacancyPct.2016, y = med_HH_Income.2016))+
  labs(title="Philadelphia Tract Med. HH Income as a Function of Pct. Vacancy",
       subtitle = "Data: US Census Bureau, 2016 ACS 5-Year Estimates",
        x ="Pct Vacant Housing Units (0-1)", y = "Median Household Income (2016 $)")+
  plotTheme
```

# 8. Census Data with tidycensus

You can access the US Census using the `tidycensus` package - this is much faster than grabbing Census data on the web. You can grab the variables you want with or without spatial data attached. 

Even if you *only* use R for this - you're making your life a bit easier. You can grab data with `tidycensus`, write it to file (more on this in the next section) and then use it in Excel or ArcGIS.

Here's how it works - you "tell" the census, through some R Code, that you want certain variables, for certain years, and certain geographies, and it returns it to you in your R Environment.

There is a whole book on tidycensus from it's creator/maintainer (*cough cough MUSA Masterclass guest speaker 2021*) [Kyle Walker](https://walker-data.com/census-r/).

## Load the tidycensus and sf packages

To use `tidycensus` you should install and load the `tidycensus` package. We are going to grab some spatial data as well, so load the `sf` package.

If you don't have these packages installed, you should install them as follows (if you do, skip this step):

```{r install_packages, warning = FALSE, eval = FALSE}
install.packages('tidycensus')
install.packages('sf')
```

OK, if you have the packages installed on your computer, now load them into your R environment with a `library` call:

```{r setup_packages2, warning = FALSE, message = FALSE}
library(tidycensus)
library(sf)
```

You will need a "key" to access the Census API. You can find one at [their website](https://api.census.gov/data/key_signup.html).

Paste it into the code block below and run it to load your key in to your R environment:

```{r load_key_hide, warning= FALSE, include=FALSE}
census_key <- read.table("~/GitHub/census_key.txt", quote="\"", comment.char="")
census_api_key(census_key[1] %>% as.character(), overwrite = TRUE)
```

```{r load_key, warning = FALSE, eval = FALSE}
census_api_key("YOUR KEY GOES HERE", overwrite = TRUE)
```

## Load census data dictionaries

Now that we have our census credentials loaded, we can start downloading information from the API using some functions from tidycensus. We are going to grab some 2016 ACS estimates for Philadelphia census tracts. In order to choose variables of interest, we are going to load the 2016 ACS data dictionary using the tidycensus function `load_variables`. We turn it into a dataframe called `acs_variable_list.2016`.

```{r load_variables, cache = TRUE}
acs_variable_list.2016 <- load_variables(2016, #year
                                         "acs5", #five year ACS estimates
                                         cache = TRUE)
```

Once we have loaded these data, we can observe and search through the data frames of variable information which should appear in our global environment either by clicking on them or using the `View(acs_variable_list.2016)` command.

Look around in th data frame for a few minutes and see what's in there.

## Downloading Data from Tidycensus

## Create a vector of census variables

We can populate a vector of variable names we will send to the Census API. We call this list `acs_vars`. This is the beauty of a code-based workflow - you can take this vector and put anything you want in it when you have a new analysis to do and re-run it for different variables. These need to be character strings, and hence, in quotes as you see below.

Keep in mind the categories and code numbers change a bit over time - you may need separate vectors for different census years.

```{r acs_vars}
acs_vars <- c("B01001_001E", # ACS total Pop estimate
              "B25002_001E", # Estimate of total housing units
              "B25002_003E", # Number of vacant housing units
              "B19013_001E", # Median HH Income ($)
              "B02001_002E", # People describing themselves as "white alone"
              "B06009_006E") # Total graduate or professional degree
```

## Call the Census API

We use the `get_acs` function in `tidycensus` to query the API and get tract level-data for all of Philadelphia from the 2016 American Community Survey's (ACS) 5-year estimates. Notice the different arguments for the function, and that they require certain types of info. For example, `geography` requires one of a finite list of answers, and they have to be formatted as character string.

Remember the `??` function - you can learn about the parameters for `get_acs` this way. There is also a function called `get_decennial` which you can use for decennial census counts.

We ask for data on our `acs_vars` for all tracts in Philadelphia County, PA in 2016. We ask for "wide" data (e.g. one variable per column, one row per tract) and we set `geometry` to `FALSE`. (Later, we will set `geometry` to `true` and get some spatial data.)

```{r get_acs_2016, cache = TRUE, message = FALSE, warning = FALSE}
acsTractsPHL.2016 <- get_acs(geography = "tract",
                             year = 2016, 
                             variables = acs_vars, 
                             geometry = FALSE, 
                             state = "PA", 
                             county = "Philadelphia", 
                             output = "wide") 
```

Voila - now you have census data. Take a look and see what you have there - since it's the ACS, they are estimates - so you'll have data marked "E" for estimate, or "M" for margin of error.

Let's use a `glimpse` command to check it out

```{r glimpse_census}
glimpse(acsTractsPHL.2016)
```

## Clean Census Data

The variable names that come with the stock Census data aren't that helpful.

You can use the `rename` function from `dplyr` to make the columns in your data intelligible.

```{r do_some_dplyr, cache = TRUE}
acsTractsPHL.2016 <- acsTractsPHL.2016 %>%
  rename (total_pop.2016 = B01001_001E,
          total_HU.2016 = B25002_001E,
          total_vacant.2016 = B25002_003E,
          med_HH_Income.2016 = B19013_001E,
          total_White.2016 = B02001_002E,
          total_GradDeg.2016 = B06009_006E)
```

# 9. Vector GIS With sf

We can use R as a GIS and easily manipulate vector data (points, lines and polygons). The main package we use for this is called `sf`. Here are some things that it does:

- Reads .shps, geojson and other spatial data files.

- Allows you to manipulate spatial data using the same tools we use to manipulate data frames - `dplyr` tools like `mutate`, `rename` etc.,

- Lets you do geoprocessing like you do in ArcGIS - spatial joins, area and distance calculations, union and dissolve functions, and more.

- Makes maps by interfacing with `ggplot`

## Load in some spatial data

Let's load in some spatial data in two ways.

First, let's just load in some simple data using `st_read`, that's the `sf` package's basic read function. You can read stuff from the web or from a filepath on your computer this way.

These are some data I grabbed from Open Street Map - Philadelphia area restaurants. You might recognize some of these. If you want to know more about grabbing OSM data, [check my repo with sample code.](https://github.com/mafichman/osm_data)

These data are points (vector data can be points, lines or polygons).

```{r load_geojson, message = FALSE, warning = FALSE, results = 'hide'}

restaurants <- st_read("https://raw.githubusercontent.com/mafichman/R_FAQ_For_Planners/main/data/restaurants.geojson")

```

We can also get census data from `tidycensus` in spatial form.

You might remember the code block below from Section 8 of this demo - where we grabbed a table of census data. We can grab the same data but in spatial form by changing the `geometry` argument to `TRUE`.

These are polygon data - each row is a census tract, each column is a variable associated with each tract.

```{r get_acs_2016.sf, message=FALSE, warning=FALSE, cache=TRUE, results = 'hide'}
acsTractsPHL.2016.sf <- get_acs(geography = "tract",
                             year = 2016, 
                             variables = acs_vars, 
                             geometry = TRUE, 
                             state = "PA", 
                             county = "Philadelphia", 
                             output = "wide") %>%
  rename (total_pop.2016 = B01001_001E,
          total_HU.2016 = B25002_001E,
          total_vacant.2016 = B25002_003E,
          med_HH_Income.2016 = B19013_001E,
          total_White.2016 = B02001_002E,
          total_GradDeg.2016 = B06009_006E) 
```

## Understanding sf Objects - Geometry and Projections

Let's examine an sf object, `restaurants`. Call the object in your console and look at some of the characteristics.

It is worth noting here that the 10th row in the data set, "Mi Puebla", is a  fantastic Mexican spot in Mt Airy. It makes me hungry just seeing that data point.


```{r examine_sf}

restaurants

```

This looks a lot like a data frame, and it is! You can easily manipulate sf objects just like data frames with `dplyr` like this:

`sf_object <- sf_object %>% mutate(column_a = column_b + column_c)`

But there are a couple major differences between sf objects and data frames.

First, there is the `geometry` column - this is a set of drawing and geoprocessing instructions. You can't do a `mutate` or other kind of data manipulation on this column. `restaurants` is a point file, so the geometry is an ordered series of points.

Second, there is a `crs` or "coordinate reference system" associated with the file. If you've used GIS you've heard of "projection" - that's where it's stored in the sf object.

The crs here is 4326, which is also known as "Web Mercator" or"WGS 84" - this is the common latitude/longitude coordinate system you might be familiar with. You can change the crs when you are doing geoprocessing (more on that in a moment).

The `st_transform` command is your friend, that's how you do reprojections - look it up - `??st_transform`

The universe of coordinate systems can be found on a great website called [spatialreference.org](http://spatialreference.org).

## Making Maps With ggplot

`ggplot` supports a geometry called `geom_sf` that allows you to plot sf objects. As with making graphs and plots, you have lots of styling options with ggplot.

Here is a simple one, using the census data we grabbed:

```{r ggplot_sf1}
ggplot()+
  geom_sf(data = acsTractsPHL.2016.sf)

```

You can can make very complex maps in ggplot. You can add many geom_sf drawing layers, and you can use ggplot's `aes` features to make cool aesthetics for your maps. Here's something a bit more complicated: 

- I use a `fill` parameter in my `aes` call to symbologize `total_GradDeg.2016` in the census data.

- I set the linework to be transparent (this is outside the `aes` call because it doesn't pertain to any data, just looks.

- I add my Philly area restaurants on top, and give them a color and a size parameter.

```{r ggplot_sf2}
ggplot()+
  geom_sf(data = acsTractsPHL.2016.sf, 
          aes(fill = total_GradDeg.2016),
          color = "transparent")+
  geom_sf(data = restaurants, color = "yellow", size = 0.5)
```
 
## Vector Geoprocessing

The `sf` package has lots of commands that do basic geoprocessing. I can do spatial joins, I can calculate distances and areas, I can do spatial processes like buffer, union, and dissolve.

Mind your projections when you do these things. If you try to calculate distance if your crs is in decimal degrees, you'll get outputs in decimal degrees (probably not helpful).

### Spatial Join

You can join spatial objects to one another in `sf` just like in any other GIS. Let's join `restaurants` to `acsTracts.PHL.2016.sf`. After we do that join, we can summarize how many restaurants are in each tract.

Before we get started, we have to make sure our data are projected in the same coordinate system. Let's use the `st_crs` command to check on that. Turns out `restaurants` is in WGS 84 (crs = 4326), and `acsTractsPHL.2016.sf` is in NAD 83 (crs = 4249)

```{r}
st_crs(restaurants) == st_crs(acsTractsPHL.2016.sf)
```

The function `st_join` does the work for us here to join points to the polygons that contain them, and it has lots of options. Here, we say the type of join is `intersects` as in "does this point from the left hand data set intersect the polygon from the right hand data set?" We also say `left=TRUE` - indicating we want a "left join" - preserving all items from the left hand data set, whether or not they join to the items on the right. The things that don't join (eg. outside of Philly), they don't show up.

Notice, that I pipe the `st_transform` command to `restaurants` right inside the join statement and re-project it so this operation works!

```{r spatial_join, warning = FALSE, message = FALSE}

restaurants.and.tracts <- st_join(restaurants %>%
                                    st_transform(crs = st_crs(acsTractsPHL.2016.sf)), acsTractsPHL.2016.sf,
                                  join = st_intersects,
                                  left = TRUE)

```

This operation produces a data set where the rows are restaurants, and there is a column denoting the tract it falls in.

Now we can convert to a data frame (`as.data.frame`) summarize the number of observations by census tract (`group_by`, `summarize`), join these to our tract `sf` file by `GEOID` (`left_join`).

```{r summarize_join_map}

restaurant_summary <- 
  restaurants.and.tracts %>%
  as.data.frame() %>%
  group_by(GEOID) %>%
  summarize(restaurant_sum = n())

tracts_and_restaurants <-
  left_join(acsTractsPHL.2016.sf, restaurant_summary,
            by = c("GEOID"))

ggplot()+
  geom_sf(data = tracts_and_restaurants, 
          aes(fill = restaurant_sum),
          color = "transparent")

```

### Other Spatial Operations

The `sf` package can do most of what you do with vectors in ArcGIS, including... 

- Geometric functions like `st_union`, `st_buffer`, `st_dissolve` and so on. 

- You can find a polygon centroid with `st_centroid`.

- Measurement functions: Calculate a polygon's area or perimeter (in the native units of its projection) with `st_area`. Find the distance between two `sf` objects with `st_distance`.

There are some spatial functions that you need to find other packages for, like K-nearest-neighbors functions.

Check out the `sf` "cheat sheet" to see what it can do:

[R Studio: sf cheatsheet](https://raw.githubusercontent.com/rstudio/cheatsheets/main/sf.pdf)

## Writing spatial data

You can write out any sf objects as .shp files which are readable in ArcGIS. This is especially useful if you want to just use R to grab data efficiently and want to map it elsewhere. I prefer to write geojsons - they are more widely usable and the column names are more stable.

Use the function "st_write" like so:

`st_write(my_sf_object, "my_filepath/my_filename.shp")`

# 10. Publishing Analyses with R Markdown

I will probably be asking you to turn in assignments using R Markdown. Markdown is a way to present your code-based projects as a more polished document (you're reading a markdown right now!). For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

If you want to see how markdown works - you can find a download button at the top right of this document and download the code that was used to make this.

# 11. Project Workflows - Github and R Projects

As you work on increasingly sophisticated or collaborative projects using R, you will want to figure out a way to manage your files and code, and to figure out ways to keep your code working as you improve and develop it.

Best practices in coding is a subject for a whole entire course, but here are a few simple tips to get you started.

## 11.1 R Projects

R has a "projects" file-type which can help you keep an organized file structure and working environment in R Studio. There is a file type called `.rProj` that coordinates this. You can open a new project by going to File -> New Project in R Studio. Check out the "Files" part of your R Studio window and see how you can now graphically view your files. Best of all, if you keep things contained in a project, it's more portable from computer to computer because you're just calling files inside one Project.

[Read Hadley Wickham's tutorial](https://r4ds.had.co.nz/workflow-projects.html) on how working with R Projects can help you with organization.

## 11.2 Github

Github is a code-collaboration environment that allows you to use "source control" and work on things with others without accidentally over-writing or breaking other peoples' code. Github works well with R Projects. You can "push" and "pull" code and data straight from R Studio or use a program called Github Desktop (which I prefer). You will need an account on Github to make this all work.

Here is a quick step-by-step guide to setting up a collaborative repo, an R project, and sending some code back and forth, using Github Desktop and R Studio:

- Create a new repo on your github page

- Add your github collaborator by going to settings/collaborators

- Go to your Github Desktop app, and "clone" the repo - this will populate a version on your computer in it's own folder.

- Go into R Studio, and start a new "project" - put it in the repo folder. Do some work - create a new R script, add some data.

- When you are done - go to Github Desktop, write some annotations about the changes you made to the code, and "push" the changes to the web repo.

- Your collaborator can clone the project, and then push and pull things themselves. This will help you manage changes and versions. If you keep all your code and data inside the Github folder, the file paths will all be the same and it will be easy to collaborate. You can work separately without breaking each other's code - use "branches" to make changes on your own version of the code, and then push it to the main code base later.


The finer mechanics of collaborating with Github are too complex for this document, but [check out Matt Harris' tutorial from MUSA 508 on using Github](https://github.com/mafichman/R_FAQ_For_Planners/blob/main/pdf/Week_1_Lab_Intro_to_Github(1).pdf) for detailed instructions on how to get set up and start collaborating.

### 11.2.1. Github Pages

It's a bit weird to send HTML markdowns to clients and n00bs and say "you need to DL this and open it in your browser to see it", but you can easily publish a knit html markdown to the web using github. If you name the markdown (in HTML form) `index`, you can go to your repo settings, and activate the "pages" functionality. This will publish the html file as a github.io webpage.

This page itself is an example - (check out the repo behind it)[https://github.com/mafichman/R_FAQ_For_Planners] to see how this all works!

## 11.3 Writing Readable Code

You will probably find yourself writing code that other people have to use - or that you will save and use at some time in the future. It's important to make sure you format and annotate your code so that your colleagues or your future self can use it, understand it, and troubleshoot it.

Here are some simple tips to keep your code nice and neat:

- Name your files and variables things that are intelligible. Name a dataframe `census_2011` instead of nonsense like `new_dataframe`. Don't call your functions or your data objects names that might be used elsewhere in your R environment - for example, don't try to name a dataframe `false` or `median`.

- Use spaces in your code before and after operators like parenthesis or equals signs, like this: `theMedian <- median(c(1, 4, 6))`

- Use a return at the end of each pipe when you are using tidyverse and dplyr, and use a return after each `+` when you are using ggplot2.

- Use indentations and multiple lines to split apart long functions - try to use one idea per line. R Studio does a good job of managing indentations for you automatically. 

Here's a quick example of a few of these things in action by piping together a few operations. We `mutate` a new variable called `high_income` in `acsTractsPHL.2016.sf` and send that categorical to the `fill` command in a `geom_sf` in `ggplot2` using the `sf` package. In the "bad" example we just write a big giant sentence. In the "better" example we use returns after our pipes and plus signs, and we break up our `ifelse` statement using returns to make it easier to read.

```{r eval=FALSE}
# Bad

acsTractsPHL.2016.sf %>% mutate(high_income = ifelse(med_HH_Income.2016 > 100000, "Over 100k", "Under 100k")) %>% ggplot()+ geom_sf(aes(fill = high_income)) 
```


```{r}
# Better

acsTractsPHL.2016.sf %>% 
  mutate(high_income = ifelse(med_HH_Income.2016 > 100000, 
                              "Over 100k", 
                              "Under 100k")) %>% 
  ggplot()+ 
  geom_sf(aes(fill = high_income)) 
```


Check out [https://style.tidyverse.org/index.html](the tidyverse Style Guide) for more info on how to write legible code.

## 11.4 Data management

Coming soon!

# 12. Querying APIs

Coming soon:

TBD incl - Socrata and REST API query functionalities for R, boxr and PennBox

# 13. Other Spatial Packages

Coming soon:

Mapview, Leaflet, osmdata, raster, crs_suggest

# 14. Resources and texts

This section contains online resources for learning R and troubleshooting code-based problems.

## Troubleshooting - How to ask a question

It's super important to know how to ask a good question on the internet or on the class forum about some issue you are having. Googling around is a really good way to find help - so you can google something about the error you are having, or look on [https://stackoverflow.com](https://stackoverflow.com), which is a community of people doing Q&A and troubleshooting.

You can look up details about the functions you are using in R using the `??` command in your console like this:

`??read.csv`

You can also check out the documentation about the packages, functions and the arguments they take in the Packages tab in the Viewer window in R Studio.

If you are going to ask a question on the class forum, include some code, and details about your problem - like the data sources that you tried to use and how they look. This allows somebody to reproduce the issue on their own, or at least understand it.

*A good question gives your respondent lots of background info:*

- What is the nature of the data in your question? Can you post a `glimpse` command so we can see what the column names and data types are?

- What is the code snippet that didn't run correctly?

- What kind of error message did you get?

- What packages do you have loaded?

- Did you google the question before you came and asked for help? (The answer should be yes).

Here are some examples of some bad questions, and some alternatives:

Class forum issue: 
"I tried to make this code work (pastes code block) and I got an error." 

-> 

"I ran this code block using data that look like this (show the results of a `head` or `glimpse` command) and got this error (copy and paste error message). I have the following packages installed...."


Google query:

"How do you get my file to join the other?"

->

"Spatial joins in r with sf"

## R Cheat Sheets

R Studio's website has some really good [cheat sheets](https://www.rstudio.com/resources/cheatsheets/) with simple walkthroughs of basic functions in some core packages

- [ggplot2 - graphics and plots](https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-visualization.pdf)

- [dplyr - manipulating data frames](https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-transformation.pdf)

- [sf - vector GIS](https://raw.githubusercontent.com/rstudio/cheatsheets/main/sf.pdf)

- [R markdown - document publishing, includes knitr](https://raw.githubusercontent.com/rstudio/cheatsheets/main/rmarkdown.pdf)

- [lubridate - dates and times](https://raw.githubusercontent.com/rstudio/cheatsheets/main/lubridate.pdf)

- [(readr - loading data)](https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-import.pdf)

## Style Guides

Handy tips for making your code readable

- [R Style Guide](https://google.github.io/styleguide/Rguide.html)

- [Tidyverse Style Guide](https://style.tidyverse.org/)

## Open Source GIS Resources

- [spatialreference.org](http://spatialreference.org)

## Books

If you're in a class with me - these books are likely to be among the required or supplemental readings. They are all fantastic, open source, and extremely, extremely useful. Here they are in "bookdown" form for free.

- [R for Data Science - Hadley Wickham](https://r4ds.had.co.nz/) - Text for CPLN 675

- [Public Policy Analytics - Ken Steif](https://urbanspatial.github.io/PublicPolicyAnalytics/) - Text for MUSA 508

- [Data Visualization - Kieran Healy](https://socviz.co/)

- [Geocomputation with R - Robin Lovelace](https://geocompr.robinlovelace.net/)

- [Analyzing US Census Data - Kyle Walker](https://walker-data.com/census-r/)

- [Geospatial Health Data - Paula Moraga](https://www.paulamoraga.com/book-geospatial/)

## Articles and Other Resources

- [A detailed guide to colors in data vis style guides - Lisa Charlotte Muth](https://blog.datawrapper.de/colors-for-data-vis-style-guides/)

# FAQ

Common Q&A will go here